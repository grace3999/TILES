{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine study data into single \"tidy\" data frame for subsequent processing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import groupby\n",
    "import datetime as dt\n",
    "\n",
    "#visualizing results\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import yellowbrick as yb\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_PF_clean = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/final_data/pf_final.csv'\n",
    "path_MGT_clean = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/final_data/mgt_final.csv'\n",
    "path_part_info = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/final_data/participant_info.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare participant info df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv containing participant info\n",
    "data_part_info = pd.read_csv(path_part_info)\n",
    "data_part_info = pd.DataFrame(data = data_part_info)\n",
    "#data_PF.reset_index(inplace=True)\n",
    "\n",
    "print('Original data_part_info shape:\\n', data_part_info.shape, '\\n')\n",
    "#ensure no replicate ID (211 participants in study)\n",
    "print('Original data_part_info unique IDs:\\n', data_part_info['ParticipantID'].unique().shape, '\\n')\n",
    "#how much missing data is there?\n",
    "print('Original data_part_info missing value counts:\\n', data_part_info.isnull().sum(), '\\n')\n",
    "#what is the data type of each column?\n",
    "print('Original data_part_info data types:\\n', data_part_info.info(), '\\n')\n",
    "\n",
    "data_part_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#there should only be a single entry for each of the 212 participants but data frame is 213 rows, find duplicate ParticipantIDs\n",
    "print(data_part_info['MitreID'].value_counts())\n",
    "#examine duplicate\n",
    "print(data_part_info[data_part_info['MitreID'] == 'SD1042'])\n",
    "#examine duplicate\n",
    "print(data_part_info[data_part_info['MitreID'] == 'SD1093'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate entry confirmed, delete duplicate \n",
    "data_part_info.drop_duplicates(subset='ParticipantID', keep='first', inplace=True)\n",
    "#remove SD1042 until determined correct ParticipantID etc.\n",
    "data_part_info = data_part_info[data_part_info['MitreID'] != 'SD1042']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final clean for MGT (job, health, and personality surveys) df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv from preprocessed MGT EMAs \n",
    "data_MGT = pd.read_csv(path_MGT_clean)\n",
    "data_MGT = pd.DataFrame(data = data_MGT)\n",
    "\n",
    "print('Original data_MGT shape:\\n', data_MGT.shape, '\\n')\n",
    "#ensure no replicate ID (211 participants in study)\n",
    "print('Original data_MGT unique IDs:\\n', data_MGT['Name'].unique().shape, '\\n')\n",
    "\n",
    "#what is the data type of each column?\n",
    "print('Original data_MGT data types:\\n', data_MGT.info(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dates from objects to datetimes\n",
    "data_MGT['Date'] = data_MGT['Date'].astype('datetime64[ns]')\n",
    "data_MGT['Timestamp'] = data_MGT['Timestamp'].astype('datetime64[ns]')\n",
    "data_MGT['StartDate2'] = data_MGT['StartDate2'].astype('datetime64[ns]')\n",
    "data_MGT['EndDate2'] = data_MGT['EndDate2'].astype('datetime64[ns]')\n",
    "\n",
    "#what is the data type of each column?\n",
    "print('Original data_MGT data types:\\n', data_MGT.info(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out if there are inconsistencies within participant IDs between MGT and part_info\n",
    "in_part_not_MGT = set(data_part_info['MitreID'].unique()) - set(data_MGT['Name'].unique())\n",
    "print('in_part_not_MGT', in_part_not_MGT)\n",
    "in_MGT_not_part = set(data_MGT['Name'].unique()) - set(data_part_info['MitreID'].unique())\n",
    "print('in_MGT_not_part', in_MGT_not_part)\n",
    "\n",
    "print('MGT part length', len(data_MGT['Name'].unique()))\n",
    "print('Info part length', len(data_part_info['MitreID'].unique()))\n",
    "\n",
    "data_MGT = data_MGT[(data_MGT['Name'] != 'SD1042') & (data_MGT['Name'] != 'SG1015') & (data_MGT['Name'] != 'SY1001') & (data_MGT['Name'] != 'SG1043')]\n",
    "in_MGT_not_part = set(data_MGT['Name'].unique()) - set(data_part_info['MitreID'].unique())\n",
    "print('in_MGT_not_part', in_MGT_not_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 10 rows with nan for 'Name', drop these\n",
    "print(data_MGT.shape)\n",
    "print(data_MGT[data_MGT['Name'].isnull()].shape)\n",
    "data_MGT = data_MGT[data_MGT['Name'].isnull() == False]\n",
    "print(data_MGT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the information contained in data_part_info to data_PF\n",
    "#first create new data table of data_part_info that contains the same number of rows for each participant in that is in data_PF\n",
    "#(e.g. replicate data_part_info so same length as data_PF for each participant)\n",
    "\n",
    "data_MGT = data_MGT.sort_values(by=['Name'], ascending=True)\n",
    "\n",
    "participants = data_MGT['Name'].unique()\n",
    "\n",
    "data_MGT_part = pd.DataFrame()\n",
    "\n",
    "for part in participants:\n",
    "    df_part_long = pd.concat([data_part_info[data_part_info['MitreID'] == part]]*len(data_MGT[data_MGT['Name'] ==  part]), ignore_index=True)\n",
    "    df_part_long.reset_index(inplace=True)\n",
    "    data_MGT_part_int = pd.concat([df_part_long, data_MGT[data_MGT['Name'] ==  part].reset_index()], axis = 1)\n",
    "    data_MGT_part = data_MGT_part.append(data_MGT_part_int)\n",
    "\n",
    "#confirm the two data tables are now the same lenght\n",
    "print('data_MGT and data_MGT_part are the same length:', data_MGT.shape[0] == data_MGT_part.shape[0])\n",
    "print(data_MGT.shape[0])\n",
    "print(data_MGT_part.shape[0])\n",
    "print('does the math make sense?', data_MGT_part.shape[0] == (data_MGT_part['Name'].values == data_MGT_part['MitreID'].values).sum())\n",
    "data_MGT_part.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final clean for psychological flexibility df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv from preprocessed psychologial felxibility EMAs \n",
    "data_PF = pd.read_csv(path_PF_clean)\n",
    "data_PF = pd.DataFrame(data = data_PF)\n",
    "\n",
    "print('Original data_PF_S3 shape:\\n', data_PF.shape, '\\n')\n",
    "#ensure no replicate ID (211 participants in study)\n",
    "print('Original data_PF_S3 unique IDs:\\n', data_PF['participant_id'].unique().shape, '\\n')\n",
    "#how much missing data is there?\n",
    "print('Original data_PF_S3 missing value counts:\\n', data_PF.isnull().sum(), '\\n')\n",
    "#what is the data type of each column?\n",
    "print('Original data_PF_S3 data types:\\n', data_PF.info(), '\\n')\n",
    "#what is the participant response rate across the entire study?\n",
    "print('Non-response rate for PF survey:\\n', data_PF['completed_ts_utc'].isnull().sum() / data_PF.shape[0] * 100, '%')\n",
    "#add a binary column for if survey was completed (1) or not (0)\n",
    "data_PF['completed'] = np.where(data_PF['results_updated'].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dates from objects to datetimes\n",
    "data_PF['survey_dt'] = data_PF['survey_dt'].astype('datetime64')\n",
    "data_PF['delivered_ts_utc'] = data_PF['delivered_ts_utc'].astype('datetime64[ns]')\n",
    "data_PF['started_ts_utc'] = data_PF['started_ts_utc'].astype('datetime64[ns]')\n",
    "data_PF['completed_ts_utc'] = data_PF['completed_ts_utc'].astype('datetime64[ns]')\n",
    "data_PF['ingested_ts_utc'] = data_PF['ingested_ts_utc'].astype('datetime64[ns]')\n",
    "\n",
    "#confirm change \n",
    "print(data_PF.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute time between when survey is sent and when participant starts the survey\n",
    "data_PF['start_delay'] = (data_PF['started_ts_utc'] - data_PF['delivered_ts_utc']).astype('timedelta64[s]')\n",
    "#compute time between when survey is sent and when participant starts the survey\n",
    "data_PF['time_to_complete'] = (data_PF['completed_ts_utc'] - data_PF['started_ts_utc']).astype('timedelta64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new column of only numeric corresponding to activity questions (code -1 for write in responses)\n",
    "activity_num = []\n",
    "for index, row in data_PF.iterrows():\n",
    "    try:\n",
    "        num = float(row['activity'])\n",
    "        activity_num.append(num)\n",
    "    except:\n",
    "        activity_num.append(-1)\n",
    "\n",
    "data_PF['activity_num'] = activity_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out if there are inconsistencies within participant IDs between PF and part_info\n",
    "in_part_not_PF = set(data_part_info['ParticipantID'].unique()) - set(data_PF['participant_id'].unique())\n",
    "print('in_part_not_PF', in_part_not_PF)\n",
    "in_PF_not_part = set(data_PF['participant_id'].unique()) - set(data_part_info['ParticipantID'].unique())\n",
    "print('in_PF_not_part', in_PF_not_part)\n",
    "\n",
    "print('PF part length', len(data_PF['participant_id'].unique()))\n",
    "print('Info part length', len(data_part_info['ParticipantID'].unique()))\n",
    "\n",
    "data_PF = data_PF[(data_PF['participant_id'] != '4e471779-ecbc-4b5a-b6b0-fcbfc9479faa') & (data_PF['participant_id'] != '24166136-6ee3-4521-abaf-972fbe83d15d')]\n",
    "\n",
    "in_PF_not_part = set(data_PF['participant_id'].unique()) - set(data_part_info['ParticipantID'].unique())\n",
    "print('in_PF_not_part', in_PF_not_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the information contained in data_part_info to data_PF\n",
    "#first create new data table of data_part_info that contains the same number of rows for each participant in that is in data_PF\n",
    "#(e.g. replicate data_part_info so same length as data_PF for each participant)\n",
    "\n",
    "data_PF = data_PF.sort_values(by=['participant_id'], ascending=True)\n",
    "\n",
    "participants = data_PF['participant_id'].unique()\n",
    "\n",
    "data_PF_part = pd.DataFrame()\n",
    "\n",
    "for part in participants:\n",
    "    df_part_long = pd.concat([data_part_info[data_part_info['ParticipantID'] == part]]*len(data_PF[data_PF['participant_id'] ==  part]), ignore_index=True)\n",
    "    df_part_long.reset_index(inplace=True)\n",
    "    df_part_long_int = pd.concat([df_part_long, data_PF[data_PF['participant_id'] ==  part].reset_index()], axis = 1)\n",
    "    data_PF_part = data_PF_part.append(df_part_long_int)\n",
    "\n",
    "#confirm the two data tables are now the same lenght\n",
    "print('data_PF and data_part_info_long are the same length:', data_PF.shape[0] == data_PF_part.shape[0])\n",
    "print(data_PF.shape[0])\n",
    "print(data_PF_part.shape[0])\n",
    "print('does the math make sense?', data_PF_part.shape[0] == (data_PF_part['participant_id'].values == data_PF_part['ParticipantID'].values).sum())\n",
    "data_PF_part.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconcile columns to keep and combine dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to match with PF survey\n",
    "data_MGT_part = data_MGT_part.rename({'Name': 'name', 'StartDate2': 'date_time', 'ResponseID': 'survey_id', 'surveytype': 'survey_type', 'Q_TotalDuration': 'time_to_complete'}, axis = 1)\n",
    "#add a time and date columns\n",
    "data_MGT_part['time'] = data_MGT_part['date_time'].dt.time\n",
    "data_MGT_part['date'] = data_MGT_part['date_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to match\n",
    "data_PF_part = data_PF_part.rename({'survey_dt': 'date', 'delivered_ts_utc': 'date_time'}, axis = 1)\n",
    "#add a time sent column\n",
    "data_PF_part['Timesent'] = data_PF_part['date_time'].dt.time\n",
    "#add a time and date columns\n",
    "data_PF_part['time'] = data_PF_part['date_time'].dt.time\n",
    "data_PF_part['date'] = data_PF_part['date_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select final columns and orgainze for MGT\n",
    "\n",
    "meta_data = ['MitreID', 'ParticipantID', 'PrimaryUnit', 'SmartPhone', 'Sex',\n",
    "       'Shift', 'Wave', 'survey_id', 'survey_type', 'date_time', 'date', 'time', 'Timesent', 'time_to_complete']\n",
    "\n",
    "shared_questions = ['context1', 'context2', 'context2_10_TEXT', 'context2_TEXT', 'context3',\n",
    "       'context3_7_TEXT', 'context3_TEXT', 'context4', 'context4_3_TEXT',\n",
    "       'context4_TEXT', 'pand1', 'pand2', 'pand3', 'pand4', 'pand5', 'pand6', 'pand7', 'pand8', 'pand9', 'pand10', 'pand_pand1',\n",
    "       'pand_pand2', 'pand_pand3', 'pand_pand4', 'pand_pand5',\n",
    "       'pand_pand6', 'pand_pand7', 'pand_pand8', 'pand_pand9', 'pand_pand10', 'anxiety', 'stress']\n",
    "\n",
    "personality_questions = ['bfid1', 'bfid2', 'bfid3', 'bfid4', 'bfid5',\n",
    "       'bfid6', 'bfid7', 'bfid8', 'bfid9', 'bfid10', 'bfid_bfid1',\n",
    "       'bfid_bfid2', 'bfid_bfid3', 'bfid_bfid4', 'bfid_bfid5',\n",
    "       'bfid_bfid6', 'bfid_bfid7', 'bfid_bfid8', 'bfid_bfid9', 'bfid_bfid10']\n",
    "\n",
    "job_questions = ['work', 'irbd1', 'irbd2', 'irbd3', 'irbd4', 'irbd5',\n",
    "       'irbd6', 'irbd7', 'irbd_irbd1', 'irbd_irbd2', 'irbd_irbd3',\n",
    "       'irbd_irbd4', 'irbd_irbd5', 'irbd_irbd6', 'irbd_irbd7', 'itpd1',\n",
    "       'itpd2', 'itpd3', 'itpd_itpd1', 'itpd_itpd2', 'itpd_itpd3', 'dalal1', 'dalal2', 'dalal3',\n",
    "       'dalal4', 'dalal5', 'dalal6', 'dalal7', 'dalal8', 'dalal9', 'dalal10', 'dalal11', 'dalal12',\n",
    "       'dalal13', 'dalal14', 'dalal15', 'dalal16', \n",
    "       'dalal_dalal1',\n",
    "       'dalal_dalal2', 'dalal_dalal3', 'dalal_dalal4', 'dalal_dalal5',\n",
    "       'dalal_dalal6', 'dalal_dalal7', 'dalal_dalal8', 'dalal_dalal9',  'dalal_dalal10', 'dalal_dalal11', 'dalal_dalal12',\n",
    "       'dalal_dalal13', 'dalal_dalal14', 'dalal_dalal15', 'dalal_dalal16']\n",
    "    \n",
    "health_questions = ['alc1', 'alc2_1', 'alc2_2', 'alc2_3', 'tob1', 'tob2_1', 'tob2_2', 'tob2_3',\n",
    "       'tob2_4', 'tob2_5', 'tob2_6', 'tob2_7', 'ex1_1', 'ex2_1', 'sleep_1']\n",
    "       \n",
    "    \n",
    "data_MGT_final = data_MGT_part[meta_data + shared_questions + job_questions + health_questions + personality_questions]\n",
    "data_MGT_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with duplicate column\n",
    "data_MGT_final.columns = ['MitreID', 'ParticipantID', 'PrimaryUnit', 'SmartPhone', 'Sex',\n",
    "       'Shift', 'Wave', 'survey_id', 'survey_type', 'date_time', 'date',\n",
    "       'time', 'Timesent', 'time_to_complete1', 'time_to_complete2',\n",
    "       'context1', 'context2', 'context2_10_TEXT', 'context2_TEXT',\n",
    "       'context3', 'context3_7_TEXT', 'context3_TEXT', 'context4',\n",
    "       'context4_3_TEXT', 'context4_TEXT', 'pand1', 'pand2', 'pand3',\n",
    "       'pand4', 'pand5', 'pand6', 'pand7', 'pand8', 'pand9', 'pand10',\n",
    "       'pand_pand1', 'pand_pand2', 'pand_pand3', 'pand_pand4',\n",
    "       'pand_pand5', 'pand_pand6', 'pand_pand7', 'pand_pand8',\n",
    "       'pand_pand9', 'pand_pand10', 'anxiety', 'stress', 'work', 'irbd1',\n",
    "       'irbd2', 'irbd3', 'irbd4', 'irbd5', 'irbd6', 'irbd7', 'irbd_irbd1',\n",
    "       'irbd_irbd2', 'irbd_irbd3', 'irbd_irbd4', 'irbd_irbd5',\n",
    "       'irbd_irbd6', 'irbd_irbd7', 'itpd1', 'itpd2', 'itpd3',\n",
    "       'itpd_itpd1', 'itpd_itpd2', 'itpd_itpd3', 'dalal1', 'dalal2',\n",
    "       'dalal3', 'dalal4', 'dalal5', 'dalal6', 'dalal7', 'dalal8',\n",
    "       'dalal9', 'dalal10', 'dalal11', 'dalal12', 'dalal13', 'dalal14',\n",
    "       'dalal15', 'dalal16', 'dalal_dalal1', 'dalal_dalal2',\n",
    "       'dalal_dalal3', 'dalal_dalal4', 'dalal_dalal5', 'dalal_dalal6',\n",
    "       'dalal_dalal7', 'dalal_dalal8', 'dalal_dalal9', 'dalal_dalal10',\n",
    "       'dalal_dalal11', 'dalal_dalal12', 'dalal_dalal13', 'dalal_dalal14',\n",
    "       'dalal_dalal15', 'dalal_dalal16', 'alc1', 'alc2_1', 'alc2_2',\n",
    "       'alc2_3', 'tob1', 'tob2_1', 'tob2_2', 'tob2_3', 'tob2_4', 'tob2_5',\n",
    "       'tob2_6', 'tob2_7', 'ex1_1', 'ex2_1', 'sleep_1', 'bfid1', 'bfid2',\n",
    "       'bfid3', 'bfid4', 'bfid5', 'bfid6', 'bfid7', 'bfid8', 'bfid9',\n",
    "       'bfid10', 'bfid_bfid1', 'bfid_bfid2', 'bfid_bfid3', 'bfid_bfid4',\n",
    "       'bfid_bfid5', 'bfid_bfid6', 'bfid_bfid7', 'bfid_bfid8',\n",
    "       'bfid_bfid9', 'bfid_bfid10']\n",
    "data_MGT_final = data_MGT_final.drop(['time_to_complete2'], axis = 1)\n",
    "data_MGT_final = data_MGT_final.rename({'time_to_complete1': 'time_to_complete'}, axis='columns')\n",
    "data_MGT_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_MGT_final = data_MGT_final.sort_values(by=['Wave', 'MitreID', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select final columns and orgainze for PF\n",
    "\n",
    "meta_data = ['MitreID', 'ParticipantID', 'PrimaryUnit', 'SmartPhone', 'Sex',\n",
    "       'Shift', 'Wave', 'survey_id', 'survey_type', 'date_time', 'date', 'time', 'Timesent', 'completed', 'start_delay', 'time_to_complete']\n",
    "\n",
    "questions = ['activity', 'activity_num', 'pf_03', 'pf_04', 'pf_05', 'pf_06', 'pf_07', 'pf_08',\n",
    "       'pf_09', 'pf_10', 'pf_11', 'pf_12', 'pf_13', 'pf_14', 'pf_15',\n",
    "       'pf_mgt', 'exp_0', 'exp_1', 'exp_2', 'exp_3', 'exp_4', 'exp_5',\n",
    "       'exp_6', 'exp_7', 'exp_8', 'exp_9', 'exp_10', 'exp_11', 'exp_12',\n",
    "       'exp_13', 'exp_neg', 'exp_pos', 'exp_neut']\n",
    "    \n",
    "data_PF_final = data_PF_part[meta_data + questions]\n",
    "data_PF_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_PF_final = data_PF_final.sort_values(by=['Wave', 'MitreID', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final df with all 4 surveys\n",
    "data_final = pd.DataFrame(columns=['MitreID', 'ParticipantID', 'PrimaryUnit', 'SmartPhone', 'Sex',\n",
    "       'Shift', 'Wave', 'survey_id', 'survey_type', 'date_time', 'date',\n",
    "       'time', 'Timesent', 'completed', 'start_delay', 'time_to_complete',\n",
    "       'activity', 'activity_num', 'pf_03', 'pf_04', 'pf_05', 'pf_06',\n",
    "       'pf_07', 'pf_08', 'pf_09', 'pf_10', 'pf_11', 'pf_12', 'pf_13',\n",
    "       'pf_14', 'pf_15', 'pf_mgt', 'exp_0', 'exp_1', 'exp_2', 'exp_3',\n",
    "       'exp_4', 'exp_5', 'exp_6', 'exp_7', 'exp_8', 'exp_9', 'exp_10',\n",
    "       'exp_11', 'exp_12', 'exp_13', 'exp_neg', 'exp_pos', 'exp_neut', 'context1', 'context2',\n",
    "       'context2_10_TEXT', 'context2_TEXT', 'context3', 'context3_7_TEXT',\n",
    "       'context3_TEXT', 'context4', 'context4_3_TEXT', 'context4_TEXT',\n",
    "       'pand1', 'pand2', 'pand3', 'pand4', 'pand5', 'pand6', 'pand7',\n",
    "       'pand8', 'pand9', 'pand10', 'pand_pand1', 'pand_pand2',\n",
    "       'pand_pand3', 'pand_pand4', 'pand_pand5', 'pand_pand6',\n",
    "       'pand_pand7', 'pand_pand8', 'pand_pand9', 'pand_pand10', 'anxiety',\n",
    "       'stress', 'work', 'irbd1', 'irbd2', 'irbd3', 'irbd4', 'irbd5',\n",
    "       'irbd6', 'irbd7', 'irbd_irbd1', 'irbd_irbd2', 'irbd_irbd3',\n",
    "       'irbd_irbd4', 'irbd_irbd5', 'irbd_irbd6', 'irbd_irbd7', 'itpd1',\n",
    "       'itpd2', 'itpd3', 'itpd_itpd1', 'itpd_itpd2', 'itpd_itpd3',\n",
    "       'dalal1', 'dalal2', 'dalal3', 'dalal4', 'dalal5', 'dalal6',\n",
    "       'dalal7', 'dalal8', 'dalal9', 'dalal10', 'dalal11', 'dalal12',\n",
    "       'dalal13', 'dalal14', 'dalal15', 'dalal16', 'dalal_dalal1',\n",
    "       'dalal_dalal2', 'dalal_dalal3', 'dalal_dalal4', 'dalal_dalal5',\n",
    "       'dalal_dalal6', 'dalal_dalal7', 'dalal_dalal8', 'dalal_dalal9',\n",
    "       'dalal_dalal10', 'dalal_dalal11', 'dalal_dalal12', 'dalal_dalal13',\n",
    "       'dalal_dalal14', 'dalal_dalal15', 'dalal_dalal16', 'alc1',\n",
    "       'alc2_1', 'alc2_2', 'alc2_3', 'tob1', 'tob2_1', 'tob2_2', 'tob2_3',\n",
    "       'tob2_4', 'tob2_5', 'tob2_6', 'tob2_7', 'ex1_1', 'ex2_1',\n",
    "       'sleep_1', 'bfid1', 'bfid2', 'bfid3', 'bfid4', 'bfid5', 'bfid6',\n",
    "       'bfid7', 'bfid8', 'bfid9', 'bfid10', 'bfid_bfid1', 'bfid_bfid2',\n",
    "       'bfid_bfid3', 'bfid_bfid4', 'bfid_bfid5', 'bfid_bfid6',\n",
    "       'bfid_bfid7', 'bfid_bfid8', 'bfid_bfid9', 'bfid_bfid10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_final.append([data_MGT_final, data_PF_final], ignore_index=True, sort=False)\n",
    "data_final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create final dfs with 70 rows per participant per survey type (70 corresponds to days in study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process wave 1 data\n",
    "wave_1 = data_final[data_final['Wave'] == 1]\n",
    "wave_1 = wave_1.sort_values(by=['date'])\n",
    "#wave 1 started on 3/5/2018, remove pilot data with dates prior to start\n",
    "wave_1 = wave_1[wave_1['date'] > dt.date(2018, 3, 4)]\n",
    "print(wave_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_1_rows = pd.DataFrame(columns=wave_1.columns)\n",
    "wave_1_rows['wave_study_date'] = pd.date_range('2018-03-05', periods=71, freq='D')\n",
    "wave_1_rows['wave_study_day'] = wave_1_rows.index\n",
    "    \n",
    "final_wave1 = pd.DataFrame()\n",
    "\n",
    "for participant in wave_1['ParticipantID'].unique():\n",
    "    #get data corresponding to participant\n",
    "    data_part = wave_1[wave_1['ParticipantID'] == participant]\n",
    "    print(participant)\n",
    "    print(data_part.shape)\n",
    "    \n",
    "    #df to fill with 70 rows according to 70 dates of study\n",
    "    data_part_long = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i < wave_1_rows.shape[0]:\n",
    "        #get date\n",
    "        date = pd.date_range('2018-03-05', periods=71, freq='D')[i].date()\n",
    "        #get participant data that matches that date\n",
    "        data_part_date = data_part.loc[data_part['date'] == date]\n",
    "        #get survey date data that matches that date\n",
    "        wave_date_df = wave_1_rows.loc[wave_1_rows['wave_study_date'] == date]\n",
    "        \n",
    "        if data_part_date.shape[0] > 0:\n",
    "            data_int = data_part_date\n",
    "            #some dates had more than one survey sent \n",
    "            data_int['wave_study_date'] = wave_date_df['wave_study_date'].values.repeat(data_part_date.shape[0])\n",
    "            data_int['wave_study_day'] = wave_date_df['wave_study_day'].values.repeat(data_part_date.shape[0])\n",
    "            \n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "\n",
    "        else:\n",
    "            #fill in row corresponding to date when no survey was sent\n",
    "            data_int = wave_date_df\n",
    "            #fill in meta data for participant\n",
    "            data_int['MitreID'] = data_part['MitreID'].unique()\n",
    "            data_int['ParticipantID'] = participant\n",
    "            data_int['PrimaryUnit'] = data_part['PrimaryUnit'].unique()\n",
    "            data_int['SmartPhone'] = data_part['SmartPhone'].unique()\n",
    "            data_int['Sex'] = data_part['Sex'].unique()\n",
    "            data_int['Shift'] = data_part['Shift'].unique()\n",
    "            data_int['Wave'] = data_part['Wave'].unique()\n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "             \n",
    "        i = i + 1\n",
    "    data_part_long.reset_index(inplace=True)\n",
    "    final_wave1 = pd.concat([final_wave1, data_part_long], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process wave 2 data\n",
    "wave_2 = data_final[data_final['Wave'] == 2]\n",
    "wave_2 = wave_2.sort_values(by=['date'])\n",
    "#wave 2 started on 4/9/2018, remove pilot data with dates prior to start\n",
    "wave_2 = wave_2[wave_2['date'] > dt.date(2018, 4, 8)]\n",
    "print(wave_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_2_rows = pd.DataFrame(columns=wave_2.columns)\n",
    "wave_2_rows['wave_study_date'] = pd.date_range('2018-04-09', periods=71, freq='D')\n",
    "wave_2_rows['wave_study_day'] = wave_2_rows.index\n",
    "    \n",
    "final_wave2 = pd.DataFrame()\n",
    "\n",
    "for participant in wave_2['ParticipantID'].unique():\n",
    "    #get data corresponding to participant\n",
    "    data_part = wave_2[wave_2['ParticipantID'] == participant]\n",
    "    print(participant)\n",
    "    print(data_part.shape)\n",
    "    \n",
    "    #df to fill with 70 rows according to 70 dates of study\n",
    "    data_part_long = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i < wave_2_rows.shape[0]:\n",
    "        #get date\n",
    "        date = pd.date_range('2018-04-09', periods=71, freq='D')[i].date()\n",
    "        #get participant data that matches that date\n",
    "        data_part_date = data_part.loc[data_part['date'] == date]\n",
    "        #get survey date data that matches that date\n",
    "        wave_date_df = wave_2_rows.loc[wave_2_rows['wave_study_date'] == date]\n",
    "        \n",
    "        if data_part_date.shape[0] > 0:\n",
    "            data_int = data_part_date\n",
    "            #some dates had more than one survey sent \n",
    "            data_int['wave_study_date'] = wave_date_df['wave_study_date'].values.repeat(data_part_date.shape[0])\n",
    "            data_int['wave_study_day'] = wave_date_df['wave_study_day'].values.repeat(data_part_date.shape[0])\n",
    "            \n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "\n",
    "        else:\n",
    "            #fill in row corresponding to date when no survey was sent\n",
    "            data_int = wave_date_df\n",
    "            #fill in meta data for participant\n",
    "            data_int['MitreID'] = data_part['MitreID'].unique()\n",
    "            data_int['ParticipantID'] = participant\n",
    "            data_int['PrimaryUnit'] = data_part['PrimaryUnit'].unique()\n",
    "            data_int['SmartPhone'] = data_part['SmartPhone'].unique()\n",
    "            data_int['Sex'] = data_part['Sex'].unique()\n",
    "            data_int['Shift'] = data_part['Shift'].unique()\n",
    "            data_int['Wave'] = data_part['Wave'].unique()\n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "             \n",
    "        i = i + 1\n",
    "    data_part_long.reset_index(inplace=True)\n",
    "    final_wave2 = pd.concat([final_wave2, data_part_long], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process wave 3 data\n",
    "wave_3 = data_final[data_final['Wave'] == 3]\n",
    "wave_3 = wave_3.sort_values(by=['date'])\n",
    "#wave 3 started on 5/4/2018, remove pilot data with dates prior to start\n",
    "wave_3 = wave_3[wave_3['date'] > dt.date(2018, 5, 3)]\n",
    "print(wave_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_3_rows = pd.DataFrame(columns=wave_3.columns)\n",
    "wave_3_rows['wave_study_date'] = pd.date_range('2018-05-04', periods=71, freq='D')\n",
    "wave_3_rows['wave_study_day'] = wave_3_rows.index\n",
    "    \n",
    "final_wave3 = pd.DataFrame()\n",
    "\n",
    "for participant in wave_3['ParticipantID'].unique():\n",
    "    #get data corresponding to participant\n",
    "    data_part = wave_3[wave_3['ParticipantID'] == participant]\n",
    "    print(participant)\n",
    "    print(data_part.shape)\n",
    "    \n",
    "    #df to fill with 70 rows according to 70 dates of study\n",
    "    data_part_long = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while i < wave_3_rows.shape[0]:\n",
    "        #get date\n",
    "        date = pd.date_range('2018-04-09', periods=71, freq='D')[i].date()\n",
    "        #get participant data that matches that date\n",
    "        data_part_date = data_part.loc[data_part['date'] == date]\n",
    "        #get survey date data that matches that date\n",
    "        wave_date_df = wave_3_rows.loc[wave_3_rows['wave_study_date'] == date]\n",
    "        \n",
    "        if data_part_date.shape[0] > 0:\n",
    "            data_int = data_part_date\n",
    "            #some dates had more than one survey sent \n",
    "            data_int['wave_study_date'] = wave_date_df['wave_study_date'].values.repeat(data_part_date.shape[0])\n",
    "            data_int['wave_study_day'] = wave_date_df['wave_study_day'].values.repeat(data_part_date.shape[0])\n",
    "            \n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "\n",
    "        else:\n",
    "            #fill in row corresponding to date when no survey was sent\n",
    "            data_int = wave_date_df\n",
    "            #fill in meta data for participant\n",
    "            data_int['MitreID'] = data_part['MitreID'].unique()\n",
    "            data_int['ParticipantID'] = participant\n",
    "            data_int['PrimaryUnit'] = data_part['PrimaryUnit'].unique()\n",
    "            data_int['SmartPhone'] = data_part['SmartPhone'].unique()\n",
    "            data_int['Sex'] = data_part['Sex'].unique()\n",
    "            data_int['Shift'] = data_part['Shift'].unique()\n",
    "            data_int['Wave'] = data_part['Wave'].unique()\n",
    "            data_part_long = data_part_long.append(data_int)\n",
    "             \n",
    "        i = i + 1\n",
    "    data_part_long.reset_index(inplace=True)\n",
    "    final_wave3 = pd.concat([final_wave3, data_part_long], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_combined = pd.concat([final_wave1, final_wave2, final_wave3], axis = 0, ignore_index=True)\n",
    "print(data_final_combined.shape)\n",
    "data_final_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "data_final_combined.to_csv('data_final_combined.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

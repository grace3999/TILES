{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and clean Job, Health, and Personality EMA data from S3 bucket (in MGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path_MGT = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/Data_googledrive/MGT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of path names in MGT directory\n",
    "path_names = []\n",
    "files = os.listdir(dir_path_MGT)\n",
    "for file in files: \n",
    "        path_names.append(dir_path_MGT + \"/\" + file)\n",
    "\n",
    "len(path_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three different types of surveys contained in MGT, select example of each to examine further\n",
    "job_expl = path_names[0]\n",
    "health_expl = path_names[1]\n",
    "personality_expl = path_names[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each example into df\n",
    "job_expl_df = pd.read_csv(job_expl)\n",
    "job_expl_df = pd.DataFrame(data = job_expl_df)\n",
    "job_expl_df_columns = job_expl_df.columns.values\n",
    "print('job_expl_df', job_expl_df.shape, '\\n')\n",
    "\n",
    "health_expl_df = pd.read_csv(health_expl)\n",
    "health_expl_df = pd.DataFrame(data = health_expl_df)\n",
    "health_expl_df_columns = health_expl_df.columns.values\n",
    "print('health_expl_df', health_expl_df.shape, '\\n')\n",
    "\n",
    "personality_expl_df = pd.read_csv(personality_expl)\n",
    "personality_expl_df = pd.DataFrame(data = personality_expl_df)\n",
    "personality_expl_df_columns = personality_expl_df.columns.values\n",
    "print('personality_expl_df', personality_expl_df.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each survey had a set of common questions asked every time and then a set of questions specific to that survey type\n",
    "\n",
    "shared_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'Name', 'Date', 'DayWeek',\n",
    "       'surveytype', 'Timestamp', 'Timesent', 'Q_TotalDuration',\n",
    "       'context1', 'Context1Time_1', 'Context1Time_2', 'Context1Time_3',\n",
    "       'Context1Time_4', 'context2', 'context2_TEXT', 'Context2Time_1',\n",
    "       'Context2Time_2', 'Context2Time_3', 'Context2Time_4', 'context3',\n",
    "       'context3_TEXT', 'Context3Time_1', 'Context3Time_2',\n",
    "       'Context3Time_3', 'Context3Time_4', 'context4', 'context4_TEXT',\n",
    "       'Context4Time_1', 'Context4Time_2', 'Context4Time_3',\n",
    "       'Context4Time_4', 'stress', 'StressTime_1', 'StressTime_2',\n",
    "       'StressTime_3', 'StressTime_4', 'anxiety', 'AnxietyTime_1',\n",
    "       'AnxietyTime_2', 'AnxietyTime_3', 'AnxietyTime_4', 'pand1',\n",
    "       'pand2', 'pand3', 'pand4', 'pand5', 'pand6', 'pand7', 'pand8',\n",
    "       'pand9', 'pand10', 'PANASTime_1', 'PANASTime_2', 'PANASTime_3',\n",
    "       'PANASTime_4']\n",
    "\n",
    "personality_only_cols = ['bfid1', 'bfid2', 'bfid3', 'bfid4', 'bfid5',\n",
    "       'bfid6', 'bfid7', 'bfid8', 'bfid9', 'bfid10', 'bfidTime_1',\n",
    "       'bfidTime_2', 'bfidTime_3', 'bfidTime_4', 'Unnamed: 73']\n",
    "\n",
    "health_only_cols = ['sleep_1', 'sleepTime_1', 'sleepTime_2',\n",
    "       'sleepTime_3', 'sleepTime_4', 'ex1_1', 'ex2_1', 'exTime_1',\n",
    "       'exTime_2', 'exTime_3', 'exTime_4', 'tob1', 'tob1Time_1',\n",
    "       'tob1Time_2', 'tob1Time_3', 'tob1Time_4', 'tob2_1', 'tob2_2',\n",
    "       'tob2_3', 'tob2_4', 'tob2_5', 'tob2_6', 'tob2_7', 'tob2Time_1',\n",
    "       'tob2Time_2', 'tob2Time_3', 'tob2Time_4', 'alc1', 'alc1Time_1',\n",
    "       'alc1Time_2', 'alc1Time_3', 'alc1Time_4', 'alc2_1', 'alc2_2',\n",
    "       'alc2_3', 'alc2Time_1', 'alc2Time_2', 'alc2Time_3', 'alc2Time_4',\n",
    "       'Unnamed: 98']\n",
    "\n",
    "job_only_cols = ['work', 'WorkTime_1', 'WorkTime_2', 'WorkTime_3',\n",
    "       'WorkTime_4', 'itpd3', 'itpd1', 'itpd2', 'itpdTime_1',\n",
    "       'itpdTime_2', 'itpdTime_3', 'itpdTime_4', 'irbd1', 'irbd2',\n",
    "       'irbd3', 'irbd4', 'irbd5', 'irbd6', 'irbd7', 'irbdTime_1',\n",
    "       'irbdTime_2', 'irbdTime_3', 'irbdTime_4', 'dalal1', 'dalal2',\n",
    "       'dalal3', 'dalal4', 'dalal5', 'dalal6', 'dalal7', 'dalal8',\n",
    "       'dalal9', 'dalal10', 'dalal11', 'dalal12', 'dalal13', 'dalal14',\n",
    "       'dalal15', 'dalal16', 'dalalTime_1', 'dalalTime_2', 'dalalTime_3',\n",
    "       'dalalTime_4', 'Unnamed: 102']\n",
    "\n",
    "unique_cols = shared_cols + job_only_cols + health_only_cols + personality_only_cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3_expl_df = pd.concat([job_expl_df, health_expl_df, personality_expl_df])\n",
    "print('unique_cols length', len(unique_cols))\n",
    "print(all_3_expl_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list containing all files\n",
    "list_of_dfs = [pd.read_csv(filename) for filename in path_names]\n",
    "#combine into one pandas df\n",
    "combined_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix columns names\n",
    "combined_df = combined_df.rename({'V1': 'ResponseID', 'V2': 'ResponseSet', 'V3': 'StartDate', 'V4': 'EndDate', 'V5': 'Finished'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select out columns related for time \n",
    "time_columns = combined_df.filter(like='Time_').columns.values\n",
    "combined_df_time = combined_df[time_columns]\n",
    "print(combined_df_time.shape)\n",
    "\n",
    "combined_df_no_time = combined_df.drop(time_columns, axis=1)\n",
    "print(combined_df_no_time.shape)\n",
    "\n",
    "print('does the math add up?', combined_df.shape[1] == combined_df_no_time.shape[1] + combined_df_time.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_no_time['surveytype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows that do not have one of the three surveytypes listed (most of these are dulicate rows resulting from original pd.concat)\n",
    "combined_df_no_time = combined_df_no_time[combined_df['surveytype'] != 'surveytype']\n",
    "combined_df_no_time = combined_df_no_time[combined_df['surveytype'] != '{\"ImportId\":\"surveytype\"}']\n",
    "print(combined_df_no_time.shape)\n",
    "combined_df_no_time['surveytype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appartent duplicate column names (StartDate, EndDate, Finished), give unique names so can investigate further\n",
    "combined_df_no_time.columns = ['Date', 'DayWeek', 'DistributionChannel', 'Duration (in seconds)',\n",
    "       'EndDate1', 'Finished1', 'Name', 'Progress', 'Q_TotalDuration',\n",
    "       'RecordedDate', 'ResponseId', 'StartDate1', 'Status', 'Timesent',\n",
    "       'Timestamp', 'Unnamed: 102', 'Unnamed: 73', 'Unnamed: 98',\n",
    "       'UserLanguage', 'ResponseID', 'ResponseSet', 'StartDate2',\n",
    "       'EndDate2', 'Finished2', 'alc1', 'alc2_1', 'alc2_2', 'alc2_3',\n",
    "       'anxiety', 'bfid1', 'bfid10', 'bfid2', 'bfid3', 'bfid4', 'bfid5',\n",
    "       'bfid6', 'bfid7', 'bfid8', 'bfid9', 'bfid_bfid1', 'bfid_bfid10',\n",
    "       'bfid_bfid2', 'bfid_bfid3', 'bfid_bfid4', 'bfid_bfid5',\n",
    "       'bfid_bfid6', 'bfid_bfid7', 'bfid_bfid8', 'bfid_bfid9', 'context1',\n",
    "       'context2', 'context2_10_TEXT', 'context2_TEXT', 'context3',\n",
    "       'context3_7_TEXT', 'context3_TEXT', 'context4', 'context4_3_TEXT',\n",
    "       'context4_TEXT', 'dalal1', 'dalal10', 'dalal11', 'dalal12',\n",
    "       'dalal13', 'dalal14', 'dalal15', 'dalal16', 'dalal2', 'dalal3',\n",
    "       'dalal4', 'dalal5', 'dalal6', 'dalal7', 'dalal8', 'dalal9',\n",
    "       'dalal_dalal1', 'dalal_dalal10', 'dalal_dalal11', 'dalal_dalal12',\n",
    "       'dalal_dalal13', 'dalal_dalal14', 'dalal_dalal15', 'dalal_dalal16',\n",
    "       'dalal_dalal2', 'dalal_dalal3', 'dalal_dalal4', 'dalal_dalal5',\n",
    "       'dalal_dalal6', 'dalal_dalal7', 'dalal_dalal8', 'dalal_dalal9',\n",
    "       'ex1_1', 'ex2_1', 'irbd1', 'irbd2', 'irbd3', 'irbd4', 'irbd5',\n",
    "       'irbd6', 'irbd7', 'irbd_irbd1', 'irbd_irbd2', 'irbd_irbd3',\n",
    "       'irbd_irbd4', 'irbd_irbd5', 'irbd_irbd6', 'irbd_irbd7', 'itpd1',\n",
    "       'itpd2', 'itpd3', 'itpd_itpd1', 'itpd_itpd2', 'itpd_itpd3',\n",
    "       'pand1', 'pand10', 'pand2', 'pand3', 'pand4', 'pand5', 'pand6',\n",
    "       'pand7', 'pand8', 'pand9', 'pand_pand1', 'pand_pand10',\n",
    "       'pand_pand2', 'pand_pand3', 'pand_pand4', 'pand_pand5',\n",
    "       'pand_pand6', 'pand_pand7', 'pand_pand8', 'pand_pand9', 'sleep_1',\n",
    "       'stress', 'surveytype', 'tob1', 'tob2_1', 'tob2_2', 'tob2_3',\n",
    "       'tob2_4', 'tob2_5', 'tob2_6', 'tob2_7', 'work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variable of columns that contain meta data\n",
    "meta_data_cols = ['Date', 'DayWeek', 'DistributionChannel', 'Duration (in seconds)',\n",
    "       'EndDate1', 'Finished1', 'Name', 'Progress', 'Q_TotalDuration',\n",
    "       'RecordedDate', 'ResponseId', 'StartDate1', 'Status', 'Timesent',\n",
    "       'Timestamp', 'Unnamed: 102', 'Unnamed: 73', 'Unnamed: 98',\n",
    "       'UserLanguage', 'ResponseID', 'ResponseSet', 'StartDate2',\n",
    "       'EndDate2', 'Finished2', 'surveytype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two different columns corresponding to ResponseID ('ResponseId' and 'ResponseID') with some NaNs in each, need to reconcile and then delete duplicate\n",
    "combined_df_no_time.loc[combined_df_no_time['ResponseID'].isnull(), 'ResponseID'] = combined_df_no_time.loc[combined_df_no_time['ResponseID'].isnull()]['ResponseId'].values\n",
    "combined_df_no_time.loc[combined_df_no_time['ResponseId'].isnull(), 'ResponseId'] = combined_df_no_time.loc[combined_df_no_time['ResponseId'].isnull()]['ResponseID'].values\n",
    "\n",
    "print('does the math add up?', ((combined_df_no_time['ResponseId'] == combined_df_no_time['ResponseID']) == True).sum() == combined_df_no_time.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#explore missing values - focus on columns with metadata only\n",
    "combined_df_no_time[meta_data_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop meta param columns with majority null\n",
    "combined_df_no_time = combined_df_no_time.drop(['DistributionChannel', \n",
    "                                                'Duration (in seconds)',\n",
    "                                                'EndDate1', \n",
    "                                                'Finished1', \n",
    "                                                'Progress',\n",
    "                                                'RecordedDate', \n",
    "                                                'ResponseId', \n",
    "                                                'StartDate1',\n",
    "                                                'Status',\n",
    "                                                'Unnamed: 102', \n",
    "                                                'Unnamed: 73', \n",
    "                                                'Unnamed: 98',\n",
    "                                                'UserLanguage'], axis = 1)\n",
    "print(combined_df_no_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'R_' from ResponseID column\n",
    "combined_df_no_time['ResponseID'] = combined_df_no_time['ResponseID'].str.lstrip('R_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this info as csv to use for subsequent analysis and to share with JV and rest of group\n",
    "combined_df_no_time.to_csv('mgt_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careless responder feature engineering and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import groupby\n",
    "import datetime as dt\n",
    "import scipy as sp\n",
    "\n",
    "#visualizing results\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import yellowbrick as yb\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/final_data/final_data_complete.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv containing data from all surveys\n",
    "full_data = pd.read_pickle(data_path)\n",
    "full_data = pd.DataFrame(data = full_data)\n",
    "full_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Original data shape:\\n', full_data.shape, '\\n')\n",
    "#ensure no replicate ID (212 participants in study)\n",
    "print('Original data unique IDs:\\n', full_data['ParticipantID'].unique().shape, '\\n')\n",
    "#ensure no replicate ID (212 participants in study)\n",
    "print('Original data unique IDs:\\n', full_data['MitreID'].unique().shape, '\\n')\n",
    "#how much missing data is there?\n",
    "print('Original data missing value counts:\\n', full_data.isnull().sum(), '\\n')\n",
    "#what is the data type of each column?\n",
    "print('Original data data types:\\n', full_data.info(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['survey_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be 71\n",
    "len(full_data['wave_study_day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create study date bins\n",
    "full_data['wave_study_date_bin'] = pd.cut(full_data['wave_study_day'], 5)\n",
    "full_data['wave_study_date_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_homevsworking_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at home should not be working\n",
    "    if (row[location_col] == num_dic['location_home']) & (row[activity_col] == num_dic['activity_work']):\n",
    "        context_homevsworking = 1\n",
    "    else:\n",
    "        context_homevsworking = 0\n",
    "\n",
    "    return context_homevsworking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_workvsactivities_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at work should not be playing sports, household activities, civic duties\n",
    "    if (row[location_col] == num_dic['location_work']) & \\\n",
    "    ((row[activity_col] == num_dic['activity_sports']) | (row[activity_col] == num_dic['activity_household']) | (row[activity_col] == num_dic['activity_civic'])):\n",
    "        context_workvsactivities = 1\n",
    "    else:\n",
    "        context_workvsactivities = 0\n",
    "\n",
    "    return context_workvsactivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_workvswork_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at work should be working\n",
    "    if (row[location_col] == num_dic['location_work']) & (row[activity_col] != num_dic['activity_work']):\n",
    "        context_workvswork = 1\n",
    "    else:\n",
    "        context_workvswork = 0\n",
    "\n",
    "    return context_workvswork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_drivevsdrive_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at vehicle should be driving/travel    \n",
    "    if (row[location_col] == num_dic['location_vehicle']) & (row[activity_col] != num_dic['activity_drive']):\n",
    "        context_drivevsdrive = 1\n",
    "    else:\n",
    "        context_drivevsdrive = 0\n",
    "\n",
    "    return context_drivevsdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].dropna().values\n",
    "    b = row.loc[param3:param4].dropna().values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        mean_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            mean_diff = np.abs(np.mean(a) -  np.mean(b))\n",
    "        except:\n",
    "            print(row)\n",
    "            mean_diff = np.nan\n",
    "            \n",
    "    return mean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].values\n",
    "    b = row.loc[param3:param4].values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        median_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            median_diff = np.abs(np.median(a) -  np.median(b))\n",
    "        except:\n",
    "            print(row)\n",
    "            median_diff = np.nan\n",
    "        \n",
    "    return median_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].values\n",
    "    b = row.loc[param3:param4].values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        mode_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            mode_diff = np.abs(sp.stats.mode(a)[0][0] -  sp.stats.mode(b)[0][0])\n",
    "        except:\n",
    "            print(row)\n",
    "            mode_diff = np.nan\n",
    "    \n",
    "    return mode_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longstring_CR(row):\n",
    "    #create features related to long string analysis (feature of how long the string is and feature of what the string consisted of)\n",
    "\n",
    "    groups = groupby(row)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "    max_pair = max(result, key=lambda x:x[1])\n",
    "    max_string_length = max_pair[1]\n",
    "    max_answer = max_pair[0]\n",
    "\n",
    "    if max_string_length == 1:\n",
    "        max_answer = 0\n",
    "    return pd.Series((max_string_length, max_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_CR(row):\n",
    "    \n",
    "    #auc\n",
    "    auc = np.trapz(row.dropna())\n",
    "    \n",
    "    #std\n",
    "    std = np.std(row.dropna())\n",
    "    \n",
    "    #skew\n",
    "    if std == 0.0:\n",
    "        skew = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            skew = sp.stats.skew(row.dropna())\n",
    "        except:\n",
    "            skew = np.nan\n",
    "    \n",
    "    #kurtosis\n",
    "    if std == 0.0:\n",
    "        kurt = -3.0\n",
    "    else:\n",
    "        try:\n",
    "            kurt = sp.stats.kurtosis(row.dropna())\n",
    "        except:\n",
    "            kurt = np.nan\n",
    "            \n",
    "    return pd.Series((auc, std, skew, kurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_seeded_CR(row):\n",
    "    \n",
    "    #auc\n",
    "    auc_seeded = np.trapz(np.append(row.dropna().values, 0.0))\n",
    "    \n",
    "    #std\n",
    "    std_seeded = np.std(np.append(row.dropna().values, 0.0))\n",
    "\n",
    "    #skew\n",
    "    if std_seeded == 0.0:\n",
    "        skew_seeded = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            skew_seeded = sp.stats.skew(np.append(row.dropna().values, 0.0))\n",
    "        except:\n",
    "            skew_seeded = np.nan\n",
    "    \n",
    "    #kurtosis\n",
    "    if std_seeded == 0.0:\n",
    "        kurt_seeded = -3.0\n",
    "    else:\n",
    "        try:\n",
    "            kurt_seeded = sp.stats.kurtosis(np.append(row.dropna().values, 0.0))\n",
    "        except:\n",
    "            kurt_seeded = np.nan\n",
    "            \n",
    "    return pd.Series((auc_seeded, std_seeded, skew_seeded, kurt_seeded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CR feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Engage surveys\n",
    "\n",
    "Context question\n",
    "- Semantic Antonyms\n",
    "    - if context1 = home (0), then context2 ≠ work and work related (0)\n",
    "    - if context1 = work (1), then context2 ≠ leisure sports (4), household activities (7), org/civic (11)\n",
    "- Semantic Synonyms\n",
    "    - if context1 = work (1), then context2 most likely work and work related (0)\n",
    "    - If context1 = vehicle (4), then context2 most likely travel or commute (12)\n",
    "- Internal consistency\n",
    "    - if context1 = 5 (other) then should have a write in\n",
    "    - if context2 = 13 (other) then should have a write in\n",
    "\n",
    "Longstring\n",
    "- All questions use same scale (1=not at all, 7=very much), but there are 5 different constructs assessed\n",
    "\n",
    "Semantic consistency\n",
    "- Internal consistency (within construct) should be greater than consistency across constructs\n",
    "\n",
    "Semantic synonyms \n",
    "- not applicable \n",
    "\n",
    "Semantic antonyms\n",
    "- Hindrance stressors should be negatively correlated with support \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed engage and related columns\n",
    "engage_only = full_data[(full_data['survey_type'] == 'engage_psycap') & (full_data['completed'] == 1.0)]\n",
    "\n",
    "print(engage_only.shape)\n",
    "engage_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_dic = {'location_home': 0, 'location_work': 1, 'location_vehicle': 4,\n",
    "           'activity_work': 0, 'activity_sports': 4, 'activity_household': 7, 'activity_civic': 11, 'activity_drive': 12}\n",
    "location_col = 'engage_location'\n",
    "activity_col = 'engage_activity'\n",
    "engage_only['context_homevsworking'] = engage_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_workvsactivities'] = engage_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_workvswork'] = engage_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_drivevsdrive'] = engage_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "param1 = 'support_mgt'\n",
    "param2 = 'support_mgt'\n",
    "param3 = 'hindrance_mgt'\n",
    "param4 = 'hindrance_mgt'\n",
    "engage_only['hinderance_vs_support_mean'] = engage_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "engage_only['hinderance_vs_support_median'] = engage_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "engage_only['hinderance_vs_support_mode'] = engage_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "param1 = 'engage_3'\n",
    "param2 = 'engage_29'\n",
    "engage_only[['longstring_count', 'longstring_answer']] = engage_only.apply(lambda row: longstring_CR(row.loc[param1:param2]), axis=1)\n",
    "engage_only['longstring_mult'] = engage_only['longstring_count'] * engage_only['longstring_answer']\n",
    "\n",
    "engage_only[['auc', 'std', 'skew', 'kurt']] = engage_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "engage_only[['auc_seeded', 'std_seeded', 'skew_seeded', 'kurt_seeded']] = engage_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Psych Flex\n",
    "\n",
    "Should have answered every question\n",
    "\n",
    "Longstring\n",
    "- Legitimate longstrings of  ≥ 8 are unlikely for response “5”\n",
    "    - make column with longest string\n",
    "    - make column with number that longest string consisted of\n",
    "\n",
    "Semantic consistency\n",
    "- Legitimate scores of pf_mgt=5 are almost impossible\n",
    "\n",
    "Semantic antonyms\n",
    "- Not applicable\n",
    "\n",
    "Semantic synonyms \n",
    "- not applicable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed PF and related columns\n",
    "psych_flex_only = full_data[(full_data['survey_type'] == 'psych_flex') & (full_data['completed'] == 1.0)]\n",
    "\n",
    "print(psych_flex_only.shape)\n",
    "psych_flex_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = 'pf_03'\n",
    "param2 = 'pf_15'\n",
    "\n",
    "psych_flex_only[['longstring_count', 'longstring_answer']] = psych_flex_only.apply(lambda row: longstring_CR(row.loc[param1:param2]), axis=1)\n",
    "psych_flex_only['longstring_mult'] = psych_flex_only['longstring_count'] * psych_flex_only['longstring_answer']\n",
    "\n",
    "psych_flex_only[['auc', 'std', 'skew', 'kurt']] = psych_flex_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "psych_flex_only[['auc_seeded', 'std_seeded', 'skew_seeded', 'kurt_seeded']] = psych_flex_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_flex_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Jobs\n",
    "\n",
    "Context question (context 2 = activity, context 3 = location)\n",
    "- Semantic Antonyms\n",
    "    - if context3 = home (1), then context2 ≠ work and work related (1)\n",
    "    - if context3 = work (2), then context2 ≠ leisure sports (3), household activities (6), org/civic (10)\n",
    "- Semantic Synonyms\n",
    "    - if context3 = work (2), then context2 most likely work and work related (1)\n",
    "    - If context3 = vehicle (5), then context2 most likely travel or commute (11)\n",
    "\n",
    "Affect/Anxiety/Stress\n",
    "- Longstrings\n",
    "    - All questions use same scale\n",
    "- Semantic antonyms\n",
    "    - Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "\n",
    "Task Perfomrance\n",
    "- Longstrings\n",
    "    - IRB questions use same scale\n",
    "    - dalal questions use same scale\n",
    "- Consistency\n",
    "    - irb2, irb3, irb4 should be negatively correlated with irb6 and irb7\n",
    "    - itp1, itp2, itp3 should be negatively correlated with irb6 and irb7\n",
    "    - dalal1-8 should be negatively correlated with dalal9-dalal16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed job and related columns\n",
    "jobs_atwork_only = full_data[(full_data['survey_type'] == 'job') & (full_data['work'] == 1)]\n",
    "\n",
    "print(jobs_atwork_only.shape)\n",
    "jobs_atwork_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#context comparisons\n",
    "num_dic = {'location_home': 1, 'location_work': 2, 'location_vehicle': 5,\n",
    "           'activity_work': 1, 'activity_sports': 3, 'activity_household': 6, 'activity_civic': 10, 'activity_drive': 11}\n",
    "location_col = 'context3'\n",
    "activity_col = 'context2'\n",
    "jobs_atwork_only['context_homevsworking'] = jobs_atwork_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_workvsactivities'] = jobs_atwork_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_workvswork'] = jobs_atwork_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_drivevsdrive'] = jobs_atwork_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "param1 = 'pand1'\n",
    "param2 ='pand5'\n",
    "param3 = 'pand6'\n",
    "param4 = 'pand10'\n",
    "jobs_atwork_only['affect_posneg_mean'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['affect_posneg_median'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['affect_posneg_mode'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    " \n",
    "#pand10 should be positivitely correlated with anxiety\n",
    "param1 = 'pand10'\n",
    "param2 ='pand10'\n",
    "param3 = 'anxiety'\n",
    "param4 = 'anxiety'\n",
    "jobs_atwork_only['nervous_anxiety_mean'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['nervous_anxiety_median'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['nervous_anxiety_mode'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "  \n",
    "#irb1, irb2, irb3, irb4 should be negatively correlated with irb6 and irb7\n",
    "param1 = 'irbd1'\n",
    "param2 ='irbd4'\n",
    "param3 = 'irbd6'\n",
    "param4 = 'irbd7'\n",
    "jobs_atwork_only['irb_irb_mean'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['irb_irb_median'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['irb_irb_mode'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#itp1, itp2, itp3 should be negatively correlated with irb6 and irb7\n",
    "param1 = 'itpd1'\n",
    "param2 ='itpd3'\n",
    "param3 = 'irbd6'\n",
    "param4 = 'irbd7'\n",
    "jobs_atwork_only['irb_itp_mean'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['irb_itp_median'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['irb_itp_mode'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#dalal1-8 should be negatively correlated with dalal9-dalal16\n",
    "param1 = 'dalal1'\n",
    "param2 ='dalal8'\n",
    "param3 = 'dalal9'\n",
    "param4 = 'dalal16'\n",
    "jobs_atwork_only['dalal_posneg_mean'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['dalal_posneg_median'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "jobs_atwork_only['dalal_posneg_mode'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#affect longstring\n",
    "param1 = 'pand1'\n",
    "param2 = 'pand10'\n",
    "jobs_atwork_only[['longstring_count_affect', 'longstring_answer_affect']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only['longstring_mult_affect'] = jobs_atwork_only['longstring_count_affect'] * jobs_atwork_only['longstring_answer_affect']\n",
    "jobs_atwork_only[['auc_affect', 'std_affect', 'skew_affect', 'kurt_affect']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['auc_seeded_affect', 'std_seeded_affect', 'skew_seeded_affect', 'kurt_seeded_affect']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#irbd longstring\n",
    "param1 = 'irbd1'\n",
    "param2 = 'irbd7'\n",
    "jobs_atwork_only[['longstring_count_irbd', 'longstring_answer_irbd']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only['longstring_mult_irbd'] = jobs_atwork_only['longstring_count_irbd'] * jobs_atwork_only['longstring_answer_irbd']\n",
    "jobs_atwork_only[['auc_irbd', 'std_irbd', 'skew_irbd', 'kurt_irbd']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['auc_seeded_irbd', 'std_seeded_irbd', 'skew_seeded_irbd', 'kurt_seeded_irbd']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#dalal longstring\n",
    "param1 = 'dalal1'\n",
    "param2 = 'dalal16'\n",
    "jobs_atwork_only[['longstring_count_dalal', 'longstring_answer_dalal']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only['longstring_mult_dalal'] = jobs_atwork_only['longstring_count_dalal'] * jobs_atwork_only['longstring_answer_dalal']\n",
    "jobs_atwork_only[['auc_dalal', 'std_dalal', 'skew_dalal', 'kurt_dalal']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['auc_seeded_dalal', 'std_seeded_dalal', 'skew_seeded_dalal', 'kurt_seeded_dalal']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_atwork_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore CR across survey types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_cols = ['context_homevsworking',\n",
    "       'context_workvsactivities', 'context_workvswork',\n",
    "       'context_drivevsdrive', 'hinderance_vs_support_mean',\n",
    "       'hinderance_vs_support_median', 'hinderance_vs_support_mode',\n",
    "       'longstring_count', 'longstring_answer', 'longstring_mult', 'auc',\n",
    "       'std', 'skew', 'kurt', 'auc_seeded', 'std_seeded', 'skew_seeded',\n",
    "       'kurt_seeded', 'time_to_complete']\n",
    "\n",
    "psych_flex_cols = ['longstring_count',\n",
    "       'longstring_answer', 'longstring_mult', 'auc', 'std', 'skew',\n",
    "       'kurt', 'auc_seeded', 'std_seeded', 'skew_seeded', 'kurt_seeded', 'time_to_complete']\n",
    "\n",
    "jobs_atwork_only_cols = ['context_homevsworking',\n",
    "       'context_workvsactivities', 'context_workvswork',\n",
    "       'context_drivevsdrive', 'affect_posneg_mean',\n",
    "       'affect_posneg_median', 'affect_posneg_mode',\n",
    "       'nervous_anxiety_mean', 'nervous_anxiety_median',\n",
    "       'nervous_anxiety_mode', 'irb_irb_mean', 'irb_irb_median',\n",
    "       'irb_irb_mode', 'irb_itp_mean', 'irb_itp_median', 'irb_itp_mode',\n",
    "       'dalal_posneg_mean', 'dalal_posneg_median', 'dalal_posneg_mode',\n",
    "       'longstring_count_affect', 'longstring_answer_affect',\n",
    "       'longstring_mult_affect', 'auc_affect', 'std_affect',\n",
    "       'skew_affect', 'kurt_affect', 'auc_seeded_affect',\n",
    "       'std_seeded_affect', 'skew_seeded_affect', 'kurt_seeded_affect',\n",
    "       'longstring_count_irbd', 'longstring_answer_irbd',\n",
    "       'longstring_mult_irbd', 'auc_irbd', 'std_irbd', 'skew_irbd',\n",
    "       'kurt_irbd', 'auc_seeded_irbd', 'std_seeded_irbd',\n",
    "       'skew_seeded_irbd', 'kurt_seeded_irbd', 'longstring_count_dalal',\n",
    "       'longstring_answer_dalal', 'longstring_mult_dalal', 'auc_dalal',\n",
    "       'std_dalal', 'skew_dalal', 'kurt_dalal', 'auc_seeded_dalal',\n",
    "       'std_seeded_dalal', 'skew_seeded_dalal', 'kurt_seeded_dalal', 'time_to_complete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_only.dropna(subset=['hinderance_vs_support_mean'], inplace=True)\n",
    "engage_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_atwork_only.dropna(axis=0, inplace=True)\n",
    "jobs_atwork_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jobs_atwork_only[jobs_atwork_only['irb_irb_mean'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_CRs = jobs_atwork_only[jobs_atwork_only_cols].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_CRs.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in jobs_atwork_only_cols:\n",
    "    sns.jointplot(jobs_CRs['irb_irb_mean'], jobs_CRs[feature], kind='hex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_atwork_only[jobs_atwork_only_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR_data = full_data[general_cols]\n",
    "CR_data = pd.merge(CR_data, engage_only[engage_cols], on=['survey_id'], how='left', suffixes=('_demog', '_engage'))\n",
    "CR_data = pd.merge(CR_data, psych_flex_only[psych_flex_cols], on=['survey_id'], how='left', suffixes=('_demog', '_psy_flex'))\n",
    "CR_data = pd.merge(CR_data, jobs_atwork_only[jobs_atwork_only_cols], on=['survey_id'], how='left', suffixes=('_demog', 'jobs'))\n",
    "CR_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(engage_only[potential_features_cont], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_only_features = engage_only[['MitreID', \n",
    "                                    'hinderance_vs_support_SAD', \n",
    "                                    'longstring_count',\n",
    "                                    'longstring_mult', \n",
    "                                    'skew_seeded',\n",
    "                                    'kurt_seeded',\n",
    "                                    'time_to_complete']]\n",
    "engage_only_features.set_index('MitreID', inplace=True)\n",
    "engage_only_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "engage_survey_features_scaled = scaler.fit_transform(engage_only_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2,10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km_ss = KMeans(n_clusters=k, random_state=1)\n",
    "    km_ss.fit(engage_survey_features_scaled)\n",
    "    scores.append(silhouette_score(engage_survey_features_scaled, km_ss.labels_))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.title('PF kmeans at survey level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_km_survey = KMeans(n_clusters=2,random_state=1234)\n",
    "engage_km_survey.fit(engage_survey_features_scaled)\n",
    "print(silhouette_score(engage_survey_features_scaled, engage_km_survey.labels_))\n",
    "\n",
    "engage_only_features['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "engage_only_features_cont['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "engage_only['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "\n",
    "engage_only['kmeans_scaled_survey'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(engage_only_features_cont, hue = 'kmeans_scaled_survey', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in potential_features:\n",
    "    sns.barplot(x='kmeans_scaled_survey', y=feature, data=engage_only)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in potential_features:\n",
    "    sns.barplot(x='longest_string_count', y=feature, hue='kmeans_scaled_survey', data=engage_only)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='wave_study_date_bin', hue='kmeans_scaled_survey', data=engage_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = engage_only['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    data_part = engage_only[engage_only['MitreID'] == part]\n",
    "    sns.countplot(x='wave_study_day', hue='kmeans_scaled_survey', data=data_part)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cluster 1 % to final data\n",
    "participants = data['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    if part in engage_only_features.index:\n",
    "        perc = engage_only_features[(engage_only_features.index == part) & (engage_only_features['kmeans_scaled_survey'] == 1)].shape[0] / engage_only_features[engage_only_features.index == part].shape[0]\n",
    "\n",
    "        data.loc[data['MitreID'] == part, 'engage_CR_perc'] = perc\n",
    "        engage_only.loc[engage_only['MitreID'] == part, 'engage_CR_perc'] = perc\n",
    "    \n",
    "    else:\n",
    "        data.loc[data['MitreID'] == part, 'engage_CR_perc'] = np.nan\n",
    "        engage_only.loc[engage_only['MitreID'] == part, 'engage_CR_perc'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MitreID', y='engage_CR_perc', data=engage_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and clustering on PF surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_flex_only.dropna(subset=['longest_string_mult'], inplace=True)\n",
    "psych_flex_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz relationship and correlation across possible features\n",
    "potential_features = ['longest_string_count', 'longest_string_answer', 'longest_string_mult',\n",
    "                      'auc', 'std', 'skew', 'kurt', 'auc_seeded', 'std_seeded', 'skew_seeded', 'kurt_seeded', \n",
    "                      'time_to_complete']\n",
    "\n",
    "psych_flex_only_features_potential = psych_flex_only[potential_features]\n",
    "psych_flex_only_features_potential.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in potential_features:\n",
    "    sns.jointplot(psych_flex_only_features_potential['longest_string_count'], psych_flex_only_features_potential[feature], kind='hex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(psych_flex_only_features_potential, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_flex_only_features = psych_flex_only[['MitreID', \n",
    "                                            'longest_string_count',\n",
    "                                            'longest_string_mult',\n",
    "                                            'time_to_complete']]\n",
    "psych_flex_only_features.set_index('MitreID', inplace=True)\n",
    "psych_flex_only_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "PF_survey_features_scaled = scaler.fit_transform(psych_flex_only_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2,10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km_ss = KMeans(n_clusters=k, random_state=1)\n",
    "    km_ss.fit(PF_survey_features_scaled)\n",
    "    scores.append(silhouette_score(PF_survey_features_scaled, km_ss.labels_))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.title('PF kmeans at survey level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PF_km_survey = KMeans(n_clusters=3,random_state=1234)\n",
    "PF_km_survey.fit(PF_survey_features_scaled)\n",
    "print(silhouette_score(PF_survey_features_scaled, PF_km_survey.labels_))\n",
    "\n",
    "psych_flex_only_features_potential['kmeans_scaled_survey'] = [label for label in PF_km_survey.labels_ ]\n",
    "psych_flex_only_features['kmeans_scaled_survey'] = [label for label in PF_km_survey.labels_ ]\n",
    "psych_flex_only['kmeans_scaled_survey'] = [label for label in PF_km_survey.labels_ ]\n",
    "\n",
    "psych_flex_only['kmeans_scaled_survey'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in potential_features:\n",
    "    sns.barplot(x='kmeans_scaled_survey', y=feature, data=psych_flex_only)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(psych_flex_only_features_potential, hue = 'kmeans_scaled_survey', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cluster 1 % to final data\n",
    "participants = data['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    if part in psych_flex_only_features.index:\n",
    "        perc = psych_flex_only_features[(psych_flex_only_features.index == part) & (psych_flex_only_features['kmeans_scaled_survey'] == 0)].shape[0] / psych_flex_only_features[psych_flex_only_features.index == part].shape[0]\n",
    "\n",
    "        data.loc[data['MitreID'] == part, 'pf_CR_perc'] = perc\n",
    "        psych_flex_only.loc[psych_flex_only['MitreID'] == part, 'pf_CR_perc'] = perc\n",
    "    \n",
    "    else:\n",
    "        data.loc[data['MitreID'] == part, 'pf_CR_perc'] = np.nan\n",
    "        psych_flex_only.loc[psych_flex_only['MitreID'] == part, 'pf_CR_perc'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='wave_study_date_bin', hue='kmeans_scaled_survey', data=psych_flex_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MitreID', y='pf_CR_perc', data=psych_flex_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='pf_CR_perc', y='engage_CR_perc', data=data, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "participants = psych_flex_only['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    print(part)\n",
    "    data_part = psych_flex_only[psych_flex_only['MitreID'] == part]\n",
    "    sns.countplot(x='wave_study_day', hue='kmeans_scaled_survey', data=data_part)\n",
    "    plt.show()\n",
    "    data_part = engage_only[engage_only['MitreID'] == part]\n",
    "    sns.countplot(x='wave_study_day', hue='kmeans_scaled_survey', data=data_part)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_only.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_only.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_only.dropna(subset=['irb_itp'], inplace=True)\n",
    "job_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz relationship and correlation across possible features\n",
    "potential_features = ['affect_posneg',\n",
    "       'nervous_anxiety', 'irb_irb', 'irb_itp', 'dalal_posneg', 'time_to_complete',\n",
    "       'longest_string_count_affect', 'longest_string_answer_affect',\n",
    "       'longest_string_mult_affect', 'longest_string_count_irb',\n",
    "       'longest_string_answer_irb', 'longest_string_mult_irb',\n",
    "       'longest_string_count_dalal', 'longest_string_answer_dalal',\n",
    "       'longest_string_mult_dalal', 'context_homevsworking', 'context_workvsactivities',\n",
    "       'context_workvswork', 'context_drivevsdrive']\n",
    "\n",
    "job_only_features_potential = job_only[potential_features]\n",
    "job_only_features_potential.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw and seeded auc, std, skew and kurtosis (seeded is trying to deal with 0 skew of all same answer vs normally distributed answers)\n",
    "auc = []\n",
    "std = []\n",
    "skew = []\n",
    "kurt = []\n",
    "\n",
    "auc_seeded = []\n",
    "std_seeded = []\n",
    "skew_seeded = []\n",
    "kurt_seeded = []\n",
    "\n",
    "for index, row in psych_flex_only.iterrows():\n",
    "\n",
    "    #auc\n",
    "    num_auc = np.trapz(row.loc['pf_03':'pf_15'].dropna())\n",
    "    auc.append(num_auc)\n",
    "    num_auc_seeded = np.trapz(np.append(row.loc['pf_03':'pf_15'].dropna().values, 0.0))\n",
    "    auc_seeded.append(num_auc_seeded)\n",
    "    \n",
    "    #std\n",
    "    num_std = np.std(row.loc['pf_03':'pf_15'].dropna())\n",
    "    std.append(num_std)\n",
    "    num_std_seeded = np.std(np.append(row.loc['pf_03':'pf_15'].dropna().values, 0.0))\n",
    "    std_seeded.append(num_std_seeded)\n",
    "    \n",
    "    #skew\n",
    "    if num_std == 0.0:\n",
    "        num_skew = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            num_skew = sp.stats.skew(row.loc['pf_03':'pf_15'].dropna())\n",
    "        except:\n",
    "            num_skew = np.nan\n",
    "    skew.append(num_skew)\n",
    "    num_skew_seeded = sp.stats.skew(np.append(row.loc['pf_03':'pf_15'].dropna().values, 0.0))\n",
    "    skew_seeded.append(num_skew_seeded)\n",
    "    \n",
    "    #kurtosis\n",
    "    if num_std == 0.0:\n",
    "        num_kurt = -3.0\n",
    "    else:\n",
    "        try:\n",
    "            num_kurt = sp.stats.kurtosis(row.loc['pf_03':'pf_15'].dropna())\n",
    "        except:\n",
    "            num_kurt = np.nan\n",
    "    kurt.append(num_kurt)\n",
    "    try:\n",
    "        num_kurt_seeded = sp.stats.kurtosis(np.append(row.loc['pf_03':'pf_15'].dropna().values, 0.0))\n",
    "    except:\n",
    "        num_kurt_seeded = np.nan\n",
    "    kurt_seeded.append(num_kurt_seeded)\n",
    "\n",
    "psych_flex_only['auc'] = auc \n",
    "psych_flex_only['std'] = std    \n",
    "psych_flex_only['skew'] = skew\n",
    "psych_flex_only['kurt'] = kurt\n",
    "\n",
    "psych_flex_only['auc_seeded'] = auc_seeded \n",
    "psych_flex_only['std_seeded'] = std_seeded    \n",
    "psych_flex_only['skew_seeded'] = skew_seeded\n",
    "psych_flex_only['kurt_seeded'] = kurt_seeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features related to long string analysis (feature of how long the string is and feature of what the string consisted of)\n",
    "\n",
    "max_strings_affect = []\n",
    "max_answers_affect = []\n",
    "longstring_mult_affect = []\n",
    "\n",
    "max_strings_irb = []\n",
    "max_answers_irb = []\n",
    "longstring_mult_irb = []\n",
    "\n",
    "max_strings_dalal = []\n",
    "max_answers_dalal = []\n",
    "longstring_mult_dalal = []\n",
    "\n",
    "for index, row in job_only.iterrows():\n",
    "    \n",
    "    #affect/anxiety/stress longstring\n",
    "    groups_affect = groupby(row[['pand1', 'pand2', 'pand3', 'pand4', 'pand5', 'pand6', 'pand7', 'pand8', 'pand9', 'pand10', 'anxiety', 'stress']])\n",
    "    result_affect = [(label, sum(1 for _ in group)) for label, group in groups_affect]\n",
    "    max_pair_affect = max(result_affect, key=lambda x:x[1])\n",
    "    max_string_length_affect = max_pair_affect[1]\n",
    "    max_answer_affect = max_pair_affect[0]\n",
    "    mult_affect = max_string_length_affect*max_answer_affect\n",
    "    max_strings_affect.append(max_string_length_affect)\n",
    "    max_answers_affect.append(max_answer_affect)\n",
    "    longstring_mult_affect.append(mult_affect)\n",
    "    \n",
    "    #dalal longstring\n",
    "    groups_irb = groupby(row['irbd1':'irbd7'])\n",
    "    result_irb = [(label, sum(1 for _ in group)) for label, group in groups_irb]\n",
    "    max_pair_irb = max(result_irb, key=lambda x:x[1])\n",
    "    max_string_length_irb = max_pair_irb[1]\n",
    "    max_answer_irb = max_pair_irb[0]\n",
    "    mult_irb = max_string_length_irb*max_answer_irb\n",
    "    max_strings_irb.append(max_string_length_irb)\n",
    "    max_answers_irb.append(max_answer_irb)\n",
    "    longstring_mult_irb.append(mult_irb)\n",
    "    \n",
    "    #dalal longstring\n",
    "    groups_dalal = groupby(row['dalal1':'dalal16'])\n",
    "    result_dalal = [(label, sum(1 for _ in group)) for label, group in groups_dalal]\n",
    "    max_pair_dalal = max(result_dalal, key=lambda x:x[1])\n",
    "    max_string_length_dalal = max_pair_dalal[1]\n",
    "    max_answer_dalal = max_pair_dalal[0]\n",
    "    mult_irb = max_string_length_dalal*max_answer_dalal\n",
    "    max_strings_dalal.append(max_string_length_dalal)\n",
    "    max_answers_dalal.append(max_answer_dalal)\n",
    "    longstring_mult_dalal.append(mult_irb)\n",
    "    \n",
    "job_only['longest_string_count_affect'] = max_strings_affect\n",
    "job_only['longest_string_answer_affect'] = max_answers_affect\n",
    "job_only['longest_string_mult_affect'] = longstring_mult_affect\n",
    "\n",
    "job_only['longest_string_count_irb'] = max_strings_irb\n",
    "job_only['longest_string_answer_irb'] = max_answers_irb\n",
    "job_only['longest_string_mult_irb'] = longstring_mult_irb\n",
    "\n",
    "job_only['longest_string_count_dalal'] = max_strings_dalal\n",
    "job_only['longest_string_answer_dalal'] = max_answers_dalal\n",
    "job_only['longest_string_mult_dalal'] = longstring_mult_dalal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

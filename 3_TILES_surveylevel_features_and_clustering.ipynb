{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careless responder feature engineering and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import groupby\n",
    "import datetime as dt\n",
    "import scipy as sp\n",
    "import math\n",
    "\n",
    "#visualizing results\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import yellowbrick as yb\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Clinical projects/TILES/final_data/final_data_complete.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv containing data from all surveys\n",
    "full_data = pd.read_pickle(data_path)\n",
    "full_data = pd.DataFrame(data = full_data)\n",
    "full_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Original data shape:\\n', full_data.shape, '\\n')\n",
    "#ensure no replicate ID (212 participants in study)\n",
    "print('Original data unique IDs:\\n', full_data['ParticipantID'].unique().shape, '\\n')\n",
    "#ensure no replicate ID (212 participants in study)\n",
    "print('Original data unique IDs:\\n', full_data['MitreID'].unique().shape, '\\n')\n",
    "#how much missing data is there?\n",
    "print('Original data missing value counts:\\n', full_data.isnull().sum(), '\\n')\n",
    "#what is the data type of each column?\n",
    "print('Original data data types:\\n', full_data.info(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['survey_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be 71\n",
    "len(full_data['wave_study_day'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create study date bins\n",
    "full_data['wave_study_date_bin'] = pd.cut(full_data['wave_study_day'], 5)\n",
    "full_data['wave_study_date_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_homevsworking_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at home should not be working\n",
    "    if (row[location_col] == num_dic['location_home']) & (row[activity_col] == num_dic['activity_work']):\n",
    "        context_homevsworking = 1\n",
    "    else:\n",
    "        context_homevsworking = 0\n",
    "\n",
    "    return context_homevsworking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_workvsactivities_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at work should not be playing sports, household activities, civic duties\n",
    "    if (row[location_col] == num_dic['location_work']) & \\\n",
    "    ((row[activity_col] == num_dic['activity_sports']) | (row[activity_col] == num_dic['activity_household']) | (row[activity_col] == num_dic['activity_civic'])):\n",
    "        context_workvsactivities = 1\n",
    "    else:\n",
    "        context_workvsactivities = 0\n",
    "\n",
    "    return context_workvsactivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_workvswork_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at work should be working\n",
    "    if (row[location_col] == num_dic['location_work']) & (row[activity_col] != num_dic['activity_work']):\n",
    "        context_workvswork = 1\n",
    "    else:\n",
    "        context_workvswork = 0\n",
    "\n",
    "    return context_workvswork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_drivevsdrive_CR(row, location_col, activity_col):\n",
    "    \n",
    "    #if at vehicle should be driving/travel    \n",
    "    if (row[location_col] == num_dic['location_vehicle']) & (row[activity_col] != num_dic['activity_drive']):\n",
    "        context_drivevsdrive = 1\n",
    "    else:\n",
    "        context_drivevsdrive = 0\n",
    "\n",
    "    return context_drivevsdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].dropna().values\n",
    "    b = row.loc[param3:param4].dropna().values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        mean_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            mean_diff = np.abs(np.mean(a) -  np.mean(b))\n",
    "        except:\n",
    "            mean_diff = np.nan\n",
    "            \n",
    "    return mean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].values\n",
    "    b = row.loc[param3:param4].values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        median_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            median_diff = np.abs(np.median(a) -  np.median(b))\n",
    "        except:\n",
    "            median_diff = np.nan\n",
    "        \n",
    "    return median_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_diff(row, param1, param2, param3, param4):\n",
    "    \n",
    "    a = row.loc[param1:param2].values\n",
    "    b = row.loc[param3:param4].values\n",
    "    \n",
    "    if (len(a) | len(b)) == 0.0:\n",
    "        mode_diff = np.nan\n",
    "    else:\n",
    "        try:\n",
    "            mode_diff = np.abs(sp.stats.mode(a)[0][0] -  sp.stats.mode(b)[0][0])\n",
    "        except:\n",
    "            mode_diff = np.nan\n",
    "    \n",
    "    return mode_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longstring_CR(row, num_questions):\n",
    "    #create features related to long string analysis (feature of how long the string is and feature of what the string consisted of)\n",
    "\n",
    "    groups = groupby(row)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "    max_pair = max(result, key=lambda x:x[1])\n",
    "    max_string_length = max_pair[1]\n",
    "    max_answer = max_pair[0]\n",
    "\n",
    "    if max_string_length == 1:\n",
    "        max_answer = 0\n",
    "        \n",
    "    max_string_length_norm = max_string_length / num_questions\n",
    "    \n",
    "    return pd.Series((max_string_length, max_answer, max_string_length_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_CR(row):\n",
    "    \n",
    "    #auc\n",
    "    auc = np.trapz(row.dropna())\n",
    "    \n",
    "    #std\n",
    "    std = np.std(row.dropna())\n",
    "    \n",
    "    #skew\n",
    "    if std == 0.0:\n",
    "        skew = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            skew = sp.stats.skew(row.dropna())\n",
    "        except:\n",
    "            skew = np.nan\n",
    "    \n",
    "    #kurtosis\n",
    "    if std == 0.0:\n",
    "        kurt = -3.0\n",
    "    else:\n",
    "        try:\n",
    "            kurt = sp.stats.kurtosis(row.dropna())\n",
    "        except:\n",
    "            kurt = np.nan\n",
    "            \n",
    "    return pd.Series((auc, std, skew, kurt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_seeded_CR(row):\n",
    "    \n",
    "    #auc\n",
    "    auc_seeded = np.trapz(np.append(row.dropna().values, 0.0))\n",
    "    \n",
    "    #std\n",
    "    std_seeded = np.std(np.append(row.dropna().values, 0.0))\n",
    "\n",
    "    #skew\n",
    "    if std_seeded == 0.0:\n",
    "        skew_seeded = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            skew_seeded = sp.stats.skew(np.append(row.dropna().values, 0.0))\n",
    "        except:\n",
    "            skew_seeded = np.nan\n",
    "    \n",
    "    #kurtosis\n",
    "    if std_seeded == 0.0:\n",
    "        kurt_seeded = -3.0\n",
    "    else:\n",
    "        try:\n",
    "            kurt_seeded = sp.stats.kurtosis(np.append(row.dropna().values, 0.0))\n",
    "        except:\n",
    "            kurt_seeded = np.nan\n",
    "            \n",
    "    return pd.Series((auc_seeded, std_seeded, skew_seeded, kurt_seeded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intake_check_CR(row, param1, param2, param3):\n",
    "    \n",
    "    #If intake = yes, then SUM(intake types) must be > 0\n",
    "    if (row[param1] == 1) & (row.loc[param2:param3].sum() <= 0):\n",
    "        intake_check = 1\n",
    "    else:\n",
    "        intake_check = 0\n",
    "\n",
    "    return intake_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CR feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Engage surveys\n",
    "\n",
    "Context question\n",
    "- Semantic Antonyms\n",
    "    - if context1 = home (0), then context2 ≠ work and work related (0)\n",
    "    - if context1 = work (1), then context2 ≠ leisure sports (4), household activities (7), org/civic (11)\n",
    "- Semantic Synonyms\n",
    "    - if context1 = work (1), then context2 most likely work and work related (0)\n",
    "    - If context1 = vehicle (4), then context2 most likely travel or commute (12)\n",
    "- Internal consistency\n",
    "    - if context1 = 5 (other) then should have a write in\n",
    "    - if context2 = 13 (other) then should have a write in\n",
    "\n",
    "Longstring\n",
    "- All questions use same scale (1=not at all, 7=very much), but there are 5 different constructs assessed\n",
    "\n",
    "Semantic consistency\n",
    "- Internal consistency (within construct) should be greater than consistency across constructs\n",
    "\n",
    "Semantic synonyms \n",
    "- not applicable \n",
    "\n",
    "Semantic antonyms\n",
    "- Hindrance stressors should be negatively correlated with support \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed engage and related columns\n",
    "engage_only = full_data[(full_data['survey_type'] == 'engage_psycap') & (full_data['completed'] == 1.0)]\n",
    "\n",
    "print(engage_only.shape)\n",
    "engage_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_dic = {'location_home': 0, 'location_work': 1, 'location_vehicle': 4,\n",
    "           'activity_work': 0, 'activity_sports': 4, 'activity_household': 7, 'activity_civic': 11, 'activity_drive': 12}\n",
    "location_col = 'location_num'\n",
    "activity_col = 'activity_num'\n",
    "engage_only['context_homevsworking'] = engage_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_workvsactivities'] = engage_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_workvswork'] = engage_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "engage_only['context_drivevsdrive'] = engage_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#support_mgt should be negatively correlated with hindrance_mgt\n",
    "param1 = 'support_mgt'\n",
    "param2 = 'support_mgt'\n",
    "param3 = 'hindrance_mgt'\n",
    "param4 = 'hindrance_mgt'\n",
    "engage_only['mean_diff_hinderance_vs_support'] = engage_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#engage_only['meadian_diff_hinderance_vs_support'] = engage_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#engage_only['mode_diff_hinderance_vs_support'] = engage_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "param1 = 'engage_3'\n",
    "param2 = 'engage_29'\n",
    "num_questions = 27\n",
    "engage_only[['longstring_count_engage', 'longstring_answer_engage', 'longstring_count_norm_engage']] = engage_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "engage_only['longstring_mult_engage'] = engage_only['longstring_count_engage'] * engage_only['longstring_answer_engage']\n",
    "\n",
    "engage_only[['ls_auc_engage', 'ls_std_engage', 'ls_skew_engage', 'ls_kurt_engage']] = engage_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "engage_only[['ls_auc_seeded_engage', 'ls_std_seeded_engage', 'ls_skew_seeded_engage', 'ls_kurt_seeded_engage']] = engage_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "engage_only['longstring_count_norm_ave'] = engage_only['longstring_count_norm_engage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Psych Flex\n",
    "\n",
    "Should have answered every question\n",
    "\n",
    "Longstring\n",
    "- Legitimate longstrings of  ≥ 8 are unlikely for response “5”\n",
    "    - make column with longest string\n",
    "    - make column with number that longest string consisted of\n",
    "\n",
    "Semantic consistency\n",
    "- Legitimate scores of pf_mgt=5 are almost impossible\n",
    "\n",
    "Semantic antonyms\n",
    "- Not applicable\n",
    "\n",
    "Semantic synonyms \n",
    "- not applicable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed PF and related columns\n",
    "psych_flex_only = full_data[(full_data['survey_type'] == 'psych_flex') & (full_data['completed'] == 1.0)]\n",
    "\n",
    "print(psych_flex_only.shape)\n",
    "psych_flex_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = 'pf_03'\n",
    "param2 = 'pf_15'\n",
    "num_questions = 13\n",
    "\n",
    "psych_flex_only[['longstring_count_pf', 'longstring_answer_pf', 'longstring_count_norm_pf']] = psych_flex_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "psych_flex_only['longstring_mult_pf'] = psych_flex_only['longstring_count_pf'] * psych_flex_only['longstring_answer_pf']\n",
    "\n",
    "psych_flex_only[['ls_auc_pf', 'ls_std_pf', 'ls_skew_pf', 'ls_kurt_pf']] = psych_flex_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "psych_flex_only[['ls_auc_seeded_pf', 'ls_std_seeded_pf', 'ls_skew_seeded_pf', 'ls_kurt_seeded_pf']] = psych_flex_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "psych_flex_only['longstring_count_norm_ave'] = psych_flex_only['longstring_count_norm_pf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Jobs\n",
    "\n",
    "Context question (context 2 = activity, context 3 = location)\n",
    "- Semantic Antonyms\n",
    "    - if context3 = home (1), then context2 ≠ work and work related (1)\n",
    "    - if context3 = work (2), then context2 ≠ leisure sports (3), household activities (6), org/civic (10)\n",
    "- Semantic Synonyms\n",
    "    - if context3 = work (2), then context2 most likely work and work related (1)\n",
    "    - If context3 = vehicle (5), then context2 most likely travel or commute (11)\n",
    "\n",
    "Affect/Anxiety/Stress\n",
    "- Longstrings\n",
    "    - All questions use same scale\n",
    "- Semantic antonyms\n",
    "    - Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "\n",
    "Task Perfomrance\n",
    "- Longstrings\n",
    "    - IRB questions use same scale\n",
    "    - dalal questions use same scale\n",
    "- Consistency\n",
    "    - irb2, irb3, irb4 should be negatively correlated with irb6 and irb7\n",
    "    - itp1, itp2, itp3 should be negatively correlated with irb6 and irb7\n",
    "    - dalal1-8 should be negatively correlated with dalal9-dalal16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed (eg at work) job and related columns\n",
    "jobs_atwork_only = full_data[(full_data['survey_type'] == 'job') & (full_data['work'] == 1)]\n",
    "\n",
    "print(jobs_atwork_only.shape)\n",
    "jobs_atwork_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#update survey type column to reflect if at work or not\n",
    "jobs_atwork_only['survey_type'] = 'jobs_atwork'\n",
    "\n",
    "#context comparisons\n",
    "num_dic = {'location_home': 1, 'location_work': 2, 'location_vehicle': 5,\n",
    "           'activity_work': 1, 'activity_sports': 3, 'activity_household': 6, 'activity_civic': 10, 'activity_drive': 11}\n",
    "location_col = 'context3'\n",
    "activity_col = 'context2'\n",
    "jobs_atwork_only['context_homevsworking'] = jobs_atwork_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_workvsactivities'] = jobs_atwork_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_workvswork'] = jobs_atwork_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_atwork_only['context_drivevsdrive'] = jobs_atwork_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "param1 = 'pand1'\n",
    "param2 ='pand5'\n",
    "param3 = 'pand6'\n",
    "param4 = 'pand10'\n",
    "jobs_atwork_only['mean_diff_affect_posneg'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['median_diff_affect_posneg'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['mode_diff_affect_posneg'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    " \n",
    "#pand10 (nervousness) should be positivitely correlated with anxiety\n",
    "param1 = 'pand10'\n",
    "param2 ='pand10'\n",
    "param3 = 'anxiety'\n",
    "param4 = 'anxiety'\n",
    "jobs_atwork_only['mean_diff_nervous_anxiety'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['median_diff_nervous_anxiety'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['mode_diff_nervous_anxiety'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "  \n",
    "#irb1, irb2, irb3, irb4 should be negatively correlated with irb6 and irb7\n",
    "param1 = 'irbd1'\n",
    "param2 ='irbd4'\n",
    "param3 = 'irbd6'\n",
    "param4 = 'irbd7'\n",
    "jobs_atwork_only['mean_diff_irb_irb'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['median_diff_irb_irb'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['mode_diff_irb_irb'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#itp1, itp2, itp3 should be negatively correlated with irb6 and irb7\n",
    "param1 = 'itpd1'\n",
    "param2 ='itpd3'\n",
    "param3 = 'irbd6'\n",
    "param4 = 'irbd7'\n",
    "jobs_atwork_only['mean_diff_irb_itp'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['median_diff_irb_itp'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['mode_diff_irb_itp'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#dalal1-8 should be negatively correlated with dalal9-dalal16\n",
    "param1 = 'dalal1'\n",
    "param2 ='dalal8'\n",
    "param3 = 'dalal9'\n",
    "param4 = 'dalal16'\n",
    "jobs_atwork_only['mean_diff_dalal_posneg'] = jobs_atwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['median_diff_dalal_posneg'] = jobs_atwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_atwork_only['mode_diff_dalal_posneg'] = jobs_atwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#affect longstring\n",
    "param1 = 'pand1'\n",
    "param2 = 'pand10'\n",
    "num_questions = 10\n",
    "jobs_atwork_only[['longstring_count_affect', 'longstring_answer_affect', 'longstring_count_norm_affect']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "jobs_atwork_only['longstring_mult_affect'] = jobs_atwork_only['longstring_count_affect'] * jobs_atwork_only['longstring_answer_affect']\n",
    "jobs_atwork_only[['ls_auc_affect', 'ls_std_affect', 'ls_skew_affect', 'ls_kurt_affect']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['ls_auc_seeded_affect', 'ls_std_seeded_affect', 'ls_skew_seeded_affect', 'ls_kurt_seeded_affect']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#irbd longstring\n",
    "param1 = 'irbd1'\n",
    "param2 = 'irbd7'\n",
    "num_questions = 7\n",
    "jobs_atwork_only[['longstring_count_irbd', 'longstring_answer_irbd', 'longstring_count_norm_irbd']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "jobs_atwork_only['longstring_mult_irbd'] = jobs_atwork_only['longstring_count_irbd'] * jobs_atwork_only['longstring_answer_irbd']\n",
    "jobs_atwork_only[['ls_auc_irbd', 'ls_std_irbd', 'ls_skew_irbd', 'ls_kurt_irbd']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['ls_auc_seeded_irbd', 'ls_std_seeded_irbd', 'ls_skew_seeded_irbd', 'ls_kurt_seeded_irbd']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#dalal longstring\n",
    "param1 = 'dalal1'\n",
    "param2 = 'dalal16'\n",
    "num_questions = 16\n",
    "jobs_atwork_only[['longstring_count_dalal', 'longstring_answer_dalal', 'longstring_count_norm_dalal']] = jobs_atwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "jobs_atwork_only['longstring_mult_dalal'] = jobs_atwork_only['longstring_count_dalal'] * jobs_atwork_only['longstring_answer_dalal']\n",
    "jobs_atwork_only[['ls_auc_dalal', 'ls_std_dalal', 'ls_skew_dalal', 'ls_kurt_dalal']] = jobs_atwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_atwork_only[['ls_auc_seeded_dalal', 'ls_std_seeded_dalal', 'ls_skew_seeded_dalal', 'ls_kurt_seeded_dalal']] = jobs_atwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#find ave longstring norm\n",
    "jobs_atwork_only['longstring_count_norm_ave'] = jobs_atwork_only[['longstring_count_norm_affect', 'longstring_count_norm_irbd', 'longstring_count_norm_dalal']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off non at work job and related columns\n",
    "jobs_notatwork_only = full_data[(full_data['survey_type'] == 'job') & (full_data['work'] == 2)]\n",
    "\n",
    "print(jobs_notatwork_only.shape)\n",
    "jobs_notatwork_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#update survey type column to reflect if at work or not\n",
    "jobs_notatwork_only['survey_type'] = 'jobs_notatwork'\n",
    "\n",
    "#context comparisons\n",
    "num_dic = {'location_home': 1, 'location_work': 2, 'location_vehicle': 5,\n",
    "           'activity_work': 1, 'activity_sports': 3, 'activity_household': 6, 'activity_civic': 10, 'activity_drive': 11}\n",
    "location_col = 'context3'\n",
    "activity_col = 'context2'\n",
    "jobs_notatwork_only['context_homevsworking'] = jobs_notatwork_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_notatwork_only['context_workvsactivities'] = jobs_notatwork_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_notatwork_only['context_workvswork'] = jobs_notatwork_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "jobs_notatwork_only['context_drivevsdrive'] = jobs_notatwork_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "param1 = 'pand1'\n",
    "param2 ='pand5'\n",
    "param3 = 'pand6'\n",
    "param4 = 'pand10'\n",
    "jobs_notatwork_only['mean_diff_affect_posneg'] = jobs_notatwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_notatwork_only['median_diff_affect_posneg'] = jobs_notatwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_notatwork_only['mode_diff_affect_posneg'] = jobs_notatwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    " \n",
    "#pand10 (nervousness) should be positivitely correlated with anxiety\n",
    "param1 = 'pand10'\n",
    "param2 ='pand10'\n",
    "param3 = 'anxiety'\n",
    "param4 = 'anxiety'\n",
    "jobs_notatwork_only['mean_diff_nervous_anxiety'] = jobs_notatwork_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_notatwork_only['median_diff_nervous_anxiety'] = jobs_notatwork_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#jobs_notatwork_only['mode_diff_nervous_anxiety'] = jobs_notatwork_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#affect longstring\n",
    "param1 = 'pand1'\n",
    "param2 = 'pand10'\n",
    "num_questions = 10\n",
    "jobs_notatwork_only[['longstring_count_affect', 'longstring_answer_affect', 'longstring_count_norm_affect']] = jobs_notatwork_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "jobs_notatwork_only['longstring_mult_affect'] = jobs_notatwork_only['longstring_count_affect'] * jobs_notatwork_only['longstring_answer_affect']\n",
    "jobs_notatwork_only[['ls_auc_affect', 'ls_std_affect', 'ls_skew_affect', 'ls_kurt_affect']] = jobs_notatwork_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "jobs_notatwork_only[['ls_auc_seeded_affect', 'ls_std_seeded_affect', 'ls_skew_seeded_affect', 'ls_kurt_seeded_affect']] = jobs_notatwork_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "jobs_notatwork_only['longstring_count_norm_ave'] = jobs_notatwork_only['longstring_count_norm_affect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Health\n",
    "\n",
    "Context question (context 2 = activity, context 3 = location)\n",
    "- Semantic Antonyms\n",
    "    - if context3 = home (1), then context2 ≠ work and work related (1)\n",
    "    - if context3 = work (2), then context2 ≠ leisure sports (3), household activities (6), org/civic (10)\n",
    "- Semantic Synonyms\n",
    "    - if context3 = work (2), then context2 most likely work and work related (1)\n",
    "    - If context3 = vehicle (5), then context2 most likely travel or commute (11)\n",
    "\n",
    "Affect/Anxiety/Stress\n",
    "- Longstrings\n",
    "    - All questions use same scale\n",
    "- Semantic antonyms\n",
    "    - Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "\n",
    "Task Perfomrance\n",
    "- Longstrings\n",
    "    - n/a\n",
    "- Consistency\n",
    "    - If alc1 = yes, then SUM(alc2_1, alc2_2, alc2_3) must be > 0\n",
    "    - If tob1 = yes, then SUM(tob2_1, tob2_2, tob2_3, tob2_4, tob2_5, tob2_6, tob2_7) must be > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed job and related columns\n",
    "health_only = full_data[(full_data['survey_type'] == 'health')]\n",
    "\n",
    "print(health_only.shape)\n",
    "health_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#context comparisons\n",
    "num_dic = {'location_home': 1, 'location_work': 2, 'location_vehicle': 5,\n",
    "           'activity_work': 1, 'activity_sports': 3, 'activity_household': 6, 'activity_civic': 10, 'activity_drive': 11}\n",
    "location_col = 'context3'\n",
    "activity_col = 'context2'\n",
    "health_only['context_homevsworking'] = health_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "health_only['context_workvsactivities'] = health_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "health_only['context_workvswork'] = health_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "health_only['context_drivevsdrive'] = health_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "param1 = 'pand1'\n",
    "param2 ='pand5'\n",
    "param3 = 'pand6'\n",
    "param4 = 'pand10'\n",
    "health_only['mean_diff_affect_posneg'] = health_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#health_only['median_diff_affect_posneg'] = health_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#health_only['mode_diff_affect_posneg'] = health_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    " \n",
    "#pand10 (nervousness) should be positivitely correlated with anxiety\n",
    "param1 = 'pand10'\n",
    "param2 ='pand10'\n",
    "param3 = 'anxiety'\n",
    "param4 = 'anxiety'\n",
    "health_only['mean_diff_nervous_anxiety'] = health_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#health_only['median_diff_nervous_anxiety'] = health_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#health_only['mode_diff_nervous_anxiety'] = health_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "  \n",
    "#affect longstring\n",
    "param1 = 'pand1'\n",
    "param2 = 'pand10'\n",
    "num_questions = 10\n",
    "health_only[['longstring_count_affect', 'longstring_answer_affect', 'longstring_count_norm_affect']] = health_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "health_only['longstring_mult_affect'] = health_only['longstring_count_affect'] * health_only['longstring_answer_affect']\n",
    "health_only[['ls_auc_affect', 'ls_std_affect', 'ls_skew_affect', 'ls_kurt_affect']] = health_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "health_only[['ls_auc_seeded_affect', 'ls_std_seeded_affect', 'ls_skew_seeded_affect', 'ls_kurt_seeded_affect']] = health_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "health_only['longstring_count_norm_ave'] = health_only['longstring_count_norm_affect']\n",
    "\n",
    "#alc consistency\n",
    "param1 = 'alc1'\n",
    "param2 = 'alc2_1'\n",
    "param3 = 'alc2_3'\n",
    "health_only['intake_check_alc'] = health_only.apply(lambda row: intake_check_CR(row, param1, param2, param3), axis=1)\n",
    "\n",
    "#nicotine consistency\n",
    "param1 = 'tob1'\n",
    "param2 = 'tob2_1'\n",
    "param3 = 'tob2_7'\n",
    "health_only['intake_check_nic'] = health_only.apply(lambda row: intake_check_CR(row, param1, param2, param3), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes for CR features for Personality\n",
    "\n",
    "Context question (context 2 = activity, context 3 = location)\n",
    "- Semantic Antonyms\n",
    "    - if context3 = home (1), then context2 ≠ work and work related (1)\n",
    "    - if context3 = work (2), then context2 ≠ leisure sports (3), household activities (6), org/civic (10)\n",
    "- Semantic Synonyms\n",
    "    - if context3 = work (2), then context2 most likely work and work related (1)\n",
    "    - If context3 = vehicle (5), then context2 most likely travel or commute (11)\n",
    "\n",
    "Affect/Anxiety/Stress\n",
    "- Longstrings\n",
    "    - All questions use same scale\n",
    "- Semantic antonyms\n",
    "    - Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "\n",
    "Task Perfomrance\n",
    "- Longstrings\n",
    "    - All questions use same scale\n",
    "- Consistency - each pair should be negatively correlated (eg each pair is reverse scored)\n",
    "    - bfi1 and bfi6 (extraversion)\n",
    "    - bfi2 and bfi7 (agreeableness)\n",
    "    - bfi3 and bfi8 (conscientiousness)\n",
    "    - bfi4 and bfi9 (neuroticism)\n",
    "    - bfi5 and bfi10 (openness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split off completed job and related columns\n",
    "personality_only = full_data[(full_data['survey_type'] == 'personality')]\n",
    "\n",
    "print(personality_only.shape)\n",
    "personality_only['ParticipantID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#context comparisons\n",
    "num_dic = {'location_home': 1, 'location_work': 2, 'location_vehicle': 5,\n",
    "           'activity_work': 1, 'activity_sports': 3, 'activity_household': 6, 'activity_civic': 10, 'activity_drive': 11}\n",
    "location_col = 'context3'\n",
    "activity_col = 'context2'\n",
    "personality_only['context_homevsworking'] = personality_only.apply(lambda row: context_homevsworking_CR(row, location_col, activity_col), axis=1)\n",
    "personality_only['context_workvsactivities'] = personality_only.apply(lambda row: context_workvsactivities_CR(row, location_col, activity_col), axis=1)\n",
    "personality_only['context_workvswork'] = personality_only.apply(lambda row: context_workvswork_CR(row, location_col, activity_col), axis=1)\n",
    "personality_only['context_drivevsdrive'] = personality_only.apply(lambda row: context_drivevsdrive_CR(row, location_col, activity_col), axis=1)\n",
    "\n",
    "#Positive block (pan1-5) should be negatively correlated with negative block (pan6-10)\n",
    "param1 = 'pand1'\n",
    "param2 ='pand5'\n",
    "param3 = 'pand6'\n",
    "param4 = 'pand10'\n",
    "personality_only['mean_diff_affect_posneg'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_affect_posneg'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_affect_posneg'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    " \n",
    "#pand10 (nervousness) should be positivitely correlated with anxiety\n",
    "param1 = 'pand10'\n",
    "param2 ='pand10'\n",
    "param3 = 'anxiety'\n",
    "param4 = 'anxiety'\n",
    "personality_only['mean_diff_nervous_anxiety'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_nervous_anxiety'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_nervous_anxiety'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "    \n",
    "#bfi1 and bfi6 (extraversion) should be negatively correlated (eg each pair is reverse scored)\n",
    "param1 = 'bfid1'\n",
    "param2 ='bfid1'\n",
    "param3 = 'bfid6'\n",
    "param4 = 'bfid6'\n",
    "personality_only['mean_diff_extraversion'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_extraversion'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_extraversion'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#bfi2 and bfi7 (agreeableness) should be negatively correlated (eg each pair is reverse scored)\n",
    "param1 = 'bfid2'\n",
    "param2 ='bfid2'\n",
    "param3 = 'bfid7'\n",
    "param4 = 'bfid7'\n",
    "personality_only['mean_diff_agreeableness'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_agreeableness'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_agreeableness'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#bfi3 and bfi8 (conscientiousness) should be negatively correlated (eg each pair is reverse scored)\n",
    "param1 = 'bfid3'\n",
    "param2 ='bfid3'\n",
    "param3 = 'bfid8'\n",
    "param4 = 'bfid8'\n",
    "personality_only['mean_diff_conscientiousness'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_conscientiousness'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_conscientiousness'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#bfi4 and bfi9 (neuroticism) should be negatively correlated (eg each pair is reverse scored)\n",
    "param1 = 'bfid4'\n",
    "param2 ='bfid4'\n",
    "param3 = 'bfid9'\n",
    "param4 = 'bfid9'\n",
    "personality_only['mean_diff_neuroticism'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_neuroticism'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_neuroticism'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "#bfi5 and bfi10 (openness) should be negatively correlated (eg each pair is reverse scored)\n",
    "param1 = 'bfid5'\n",
    "param2 ='bfid5'\n",
    "param3 = 'bfid10'\n",
    "param4 = 'bfid10'\n",
    "personality_only['mean_diff_openness'] = personality_only.apply(lambda row: mean_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['median_diff_openness'] = personality_only.apply(lambda row: median_diff(row, param1, param2, param3, param4), axis=1)\n",
    "#personality_only['mode_diff_openness'] = personality_only.apply(lambda row: mode_diff(row, param1, param2, param3, param4), axis=1)\n",
    "\n",
    "\n",
    "#affect longstring\n",
    "param1 = 'pand1'\n",
    "param2 = 'pand10'\n",
    "num_questions = 10\n",
    "personality_only[['longstring_count_affect', 'longstring_answer_affect', 'longstring_count_norm_affect']] = personality_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "personality_only['longstring_mult_affect'] = personality_only['longstring_count_affect'] * personality_only['longstring_answer_affect']\n",
    "personality_only[['ls_auc_affect', 'ls_std_affect', 'ls_skew_affect', 'ls_kurt_affect']] = personality_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "personality_only[['ls_auc_seeded_affect', 'ls_std_seeded_affect', 'ls_skew_seeded_affect', 'ls_kurt_seeded_affect']] = personality_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#personality longstring\n",
    "param1 = 'bfid1'\n",
    "param2 = 'bfid10'\n",
    "num_questions = 10\n",
    "personality_only[['longstring_count_personality', 'longstring_answer_personality', 'longstring_count_norm_personality']] = personality_only.apply(lambda row: longstring_CR(row.loc[param1:param2], num_questions), axis=1)\n",
    "personality_only['longstring_mult_personality'] = personality_only['longstring_count_personality'] * personality_only['longstring_answer_affect']\n",
    "personality_only[['ls_auc_personality', 'ls_std_personality', 'ls_skew_personality', 'ls_kurt_personality']] = personality_only.apply(lambda row: moments_CR(row.loc[param1:param2]), axis=1)\n",
    "personality_only[['ls_auc_seeded_personality', 'ls_std_seeded_personality', 'ls_skew_seeded_personality', 'ls_kurt_seeded_personality']] = personality_only.apply(lambda row: moments_seeded_CR(row.loc[param1:param2]), axis=1)\n",
    "\n",
    "#find ave longstring norm\n",
    "personality_only['longstring_count_norm_ave'] = personality_only[['longstring_count_norm_affect', 'longstring_count_norm_personality']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_CR = pd.concat([engage_only, psych_flex_only, jobs_atwork_only, jobs_notatwork_only, health_only, personality_only], axis = 0, sort=False)\n",
    "surveys_CR.to_csv('surveys_CR.csv')\n",
    "surveys_CR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore CR across survey types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_CR['survey_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cols = ['context_homevsworking',\n",
    "       'context_workvsactivities', 'context_workvswork',\n",
    "       'context_drivevsdrive', 'mean_diff_affect_posneg', 'mean_diff_nervous_anxiety',\n",
    "               'longstring_count_affect', 'longstring_answer_affect',\n",
    "       'longstring_mult_affect', 'longstring_count_norm_affect', 'longstring_count_norm_ave', 'ls_auc_affect', 'ls_std_affect',\n",
    "       'ls_skew_affect', 'ls_kurt_affect', 'ls_auc_seeded_affect',\n",
    "       'ls_std_seeded_affect', 'ls_skew_seeded_affect',\n",
    "       'ls_kurt_seeded_affect']\n",
    "\n",
    "engage_specific = ['mean_diff_hinderance_vs_support',\n",
    "       'longstring_count_engage', 'longstring_answer_engage',\n",
    "       'longstring_mult_engage', 'ls_auc_engage', 'ls_std_engage',\n",
    "       'ls_skew_engage', 'ls_kurt_engage', 'ls_auc_seeded_engage',\n",
    "       'ls_std_seeded_engage', 'ls_skew_seeded_engage',\n",
    "       'ls_kurt_seeded_engage']\n",
    "\n",
    "pf_specific = [ 'longstring_count_pf',\n",
    "       'longstring_answer_pf', 'longstring_mult_pf', 'ls_auc_pf',\n",
    "       'ls_std_pf', 'ls_skew_pf', 'ls_kurt_pf', 'ls_auc_seeded_pf',\n",
    "       'ls_std_seeded_pf', 'ls_skew_seeded_pf', 'ls_kurt_seeded_pf']\n",
    "\n",
    "jobs_specific = ['mean_diff_irb_irb', 'mean_diff_irb_itp', 'mean_diff_dalal_posneg',\n",
    "     'longstring_count_irbd', 'longstring_answer_irbd', 'longstring_mult_irbd', 'ls_auc_irbd',\n",
    "       'ls_std_irbd', 'ls_skew_irbd', 'ls_kurt_irbd',\n",
    "       'ls_auc_seeded_irbd', 'ls_std_seeded_irbd', 'ls_skew_seeded_irbd',\n",
    "       'ls_kurt_seeded_irbd', 'longstring_count_dalal',\n",
    "       'longstring_answer_dalal', 'longstring_mult_dalal', 'ls_auc_dalal',\n",
    "       'ls_std_dalal', 'ls_skew_dalal', 'ls_kurt_dalal',\n",
    "       'ls_auc_seeded_dalal', 'ls_std_seeded_dalal',\n",
    "       'ls_skew_seeded_dalal', 'ls_kurt_seeded_dalal']\n",
    "\n",
    "health_specific =  ['intake_check_alc',\n",
    "       'intake_check_nic']\n",
    "\n",
    "personality_specific = ['mean_diff_extraversion',\n",
    "       'mean_diff_agreeableness', 'mean_diff_conscientiousness',\n",
    "       'mean_diff_neuroticism', 'mean_diff_openness',\n",
    "       'longstring_count_personality', 'longstring_answer_personality',\n",
    "       'longstring_mult_personality', 'ls_auc_personality',\n",
    "       'ls_std_personality', 'ls_skew_personality', 'ls_kurt_personality',\n",
    "       'ls_auc_seeded_personality', 'ls_std_seeded_personality',\n",
    "       'ls_skew_seeded_personality', 'ls_kurt_seeded_personality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.figure(figsize=(50,30))\n",
    "grid_size = math.ceil(np.sqrt(len(shared_cols)))\n",
    "for feature in shared_cols:\n",
    "    plt.subplot(grid_size, grid_size, i)\n",
    "    sns.barplot(x='survey_type', y=feature, data=surveys_CR)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(x='wave_study_day', y='mean_diff_affect_posneg', data=surveys_CR, hue='survey_type', ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='wave_study_date_bin', y='mean_diff_affect_posneg', data=surveys_CR, hue='survey_type', ci=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='wave_study_date_bin', y='longstring_count_norm_ave', data=surveys_CR, hue='survey_type', ci=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='wave_study_date_bin', y='longstring_count_norm_affect', data=surveys_CR, hue='survey_type', ci=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(engage_only.shape)\n",
    "engage_only_CRs = engage_only[engage_cols].dropna(axis=0)\n",
    "engage_only_CRs.set_index('MitreID', inplace=True)\n",
    "print(engage_only_CRs.shape)\n",
    "print(engage_only_CRs.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psych_flex_only.shape)\n",
    "psych_flex_only_CRs = psych_flex_only[psych_flex_cols].dropna(axis=0)\n",
    "psych_flex_only_CRs.set_index('MitreID', inplace=True)\n",
    "print(psych_flex_only_CRs.shape)\n",
    "print(psych_flex_only_CRs.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs_atwork_only.shape)\n",
    "jobs_atwork_only_CRs = jobs_atwork_only[jobs_atwork_only_cols].dropna(axis=0)\n",
    "jobs_atwork_only_CRs.set_index('MitreID', inplace=True)\n",
    "print(jobs_atwork_only_CRs.shape)\n",
    "print(jobs_atwork_only_CRs.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "engage_CRs_corr = engage_only_CRs.corr()\n",
    "engage_CRs_corr.to_csv('engage_CRs_corr.csv')\n",
    "engage_CRs_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psych_flex_CRs_corr = psych_flex_only_CRs.corr()\n",
    "psych_flex_CRs_corr.to_csv('psych_flex_CRs_corr.csv')\n",
    "psych_flex_CRs_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_atwork_CRs_corr = jobs_atwork_only_CRs.corr()\n",
    "#jobs_atwork_CRs_corr.to_csv('jobs_atwork_CRs_corr.csv')\n",
    "jobs_atwork_CRs_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.figure(figsize=(30,20))\n",
    "grid_size = math.ceil(np.sqrt(len(engage_cols)))\n",
    "for feature in engage_cols[1:]:\n",
    "    plt.subplot(grid_size, grid_size, i)\n",
    "    sns.distplot(engage_only_CRs[feature])\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.figure(figsize=(30,20))\n",
    "grid_size = math.ceil(np.sqrt(len(psych_flex_cols)))\n",
    "for feature in psych_flex_cols[1:]:\n",
    "    plt.subplot(grid_size, grid_size, i)\n",
    "    sns.distplot(psych_flex_only_CRs[feature])\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.figure(figsize=(30,30))\n",
    "grid_size = math.ceil(np.sqrt(len(jobs_atwork_only_cols)))\n",
    "for feature in jobs_atwork_only_cols[1:]:\n",
    "    plt.subplot(grid_size, grid_size, i)\n",
    "    sns.distplot(jobs_atwork_only_CRs[feature])\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(jobs_atwork_only_CRs[['mean_diff_affect_posneg',\n",
    "       'mean_diff_nervous_anxiety', 'mean_diff_irb_irb',\n",
    "       'mean_diff_irb_itp', 'mean_diff_dalal_posneg']], kind='reg', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(jobs_atwork_only_CRs[['longstring_count_affect',\n",
    "       'longstring_mult_affect', 'longstring_count_irbd',\n",
    "       'longstring_mult_irbd', 'longstring_count_dalal', 'longstring_mult_dalal']], kind='reg', diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in jobs_atwork_only_cols:\n",
    "    sns.jointplot(jobs_CRs['irb_irb_mean'], jobs_CRs[feature], kind='hex')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_atwork_only_CRs[['longstring_count_affect', 'longstring_count_irbd', 'longstring_count_dalal']].corr().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_select_features = ['mean_diff_hinderance_vs_support',\n",
    "       'longstring_count', 'longstring_mult', 'ls_auc', 'ls_std',\n",
    "       'ls_skew_seeded', 'ls_kurt_seeded', 'time_to_complete']\n",
    "\n",
    "psych_select_features = ['longstring_count', 'longstring_mult', 'ls_auc', 'ls_std',\n",
    "       'ls_skew_seeded', 'ls_kurt_seeded', 'time_to_complete']\n",
    "\n",
    "jobs_select_features = ['mean_diff_irb_itp', \n",
    "                    'longstring_count_affect', \n",
    "                    'longstring_mult_affect',\n",
    "                    'ls_std_affect', \n",
    "                    'ls_skew_seeded_affect', \n",
    "                    'ls_kurt_seeded_affect', \n",
    "                    'time_to_complete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#full feature set\n",
    "features_engage_full_scaled = scaler.fit_transform(engage_only_CRs)\n",
    "features_psychflex_full_scaled = scaler.fit_transform(psych_flex_only_CRs)\n",
    "features_jobs_atwork_full_scaled = scaler.fit_transform(jobs_atwork_only_CRs)\n",
    "\n",
    "#select feature set\n",
    "features_engage_select = engage_only_CRs[engage_select_features]\n",
    "features_psychflex_select = psych_flex_only_CRs[psych_select_features]\n",
    "features_jobs_atwork_select = jobs_atwork_only_CRs[jobs_select_features]\n",
    "\n",
    "features_engage_select_scaled = scaler.fit_transform(engage_only_CRs)\n",
    "features_psychflex_select_scaled = scaler.fit_transform(psych_flex_only_CRs)\n",
    "features_jobs_atwork_select_scaled = scaler.fit_transform(jobs_atwork_only_CRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2,10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km_ss = KMeans(n_clusters=k, random_state=1)\n",
    "    km_ss.fit(features_jobs_atwork_full_scaled)\n",
    "    scores.append(silhouette_score(features_jobs_atwork_select_scaled, km_ss.labels_))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.title('PF kmeans at survey level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_km_survey = KMeans(n_clusters=2,random_state=1234)\n",
    "engage_km_survey.fit(engage_survey_features_scaled)\n",
    "print(silhouette_score(engage_survey_features_scaled, engage_km_survey.labels_))\n",
    "\n",
    "engage_only_features['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "engage_only_features_cont['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "engage_only['kmeans_scaled_survey'] = [label for label in engage_km_survey.labels_ ]\n",
    "\n",
    "engage_only['kmeans_scaled_survey'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This function can take a long to run\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_features_engage_full = tsne.fit_transform(features_engage_full_scaled)\n",
    "tsne_features_psychflex_full = tsne.fit_transform(features_psychflex_full_scaled)\n",
    "tsne_features_jobs_full = tsne.fit_transform(features_jobs_atwork_full_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_engage.shape)\n",
    "tsne_features_engage_df = pd.DataFrame(data = tsne_features_engage_full, columns = ['tsne_0', 'tsne_1'], index = engage_only_CRs.index)\n",
    "tsne_features_engage_df = pd.concat([engage_only_CRs, tsne_features_engage_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_engage_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_psychflex.shape)\n",
    "tsne_features_psychflex_df = pd.DataFrame(data = tsne_features_psychflex_full, columns = ['tsne_0', 'tsne_1'], index = psych_flex_only_CRs.index)\n",
    "tsne_features_psychflex_df = pd.concat([psych_flex_only_CRs, tsne_features_psychflex_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_psychflex_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_jobs.shape)\n",
    "tsne_features_jobs_df = pd.DataFrame(data = tsne_features_jobs_full, columns = ['tsne_0', 'tsne_1'], index = jobs_atwork_only_CRs.index)\n",
    "tsne_features_jobs_df = pd.concat([jobs_atwork_only_CRs, tsne_features_jobs_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_jobs_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This function can take a long to run\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_features_engage_select = tsne.fit_transform(features_engage_select_scaled)\n",
    "tsne_features_psychflex_select = tsne.fit_transform(features_psychflex_select_scaled)\n",
    "tsne_features_jobs_select = tsne.fit_transform(features_jobs_atwork_select_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_engage.shape)\n",
    "tsne_features_engage_df = pd.DataFrame(data = tsne_features_engage_select, columns = ['tsne_0', 'tsne_1'], index = engage_only_CRs.index)\n",
    "tsne_features_engage_df = pd.concat([engage_only_CRs, tsne_features_engage_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_engage_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_psychflex.shape)\n",
    "tsne_features_psychflex_df = pd.DataFrame(data = tsne_features_psychflex_select, columns = ['tsne_0', 'tsne_1'], index = psych_flex_only_CRs.index)\n",
    "tsne_features_psychflex_df = pd.concat([psych_flex_only_CRs, tsne_features_psychflex_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_psychflex_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsne_features_jobs.shape)\n",
    "tsne_features_jobs_df = pd.DataFrame(data = tsne_features_jobs_select, columns = ['tsne_0', 'tsne_1'], index = jobs_atwork_only_CRs.index)\n",
    "tsne_features_jobs_df = pd.concat([jobs_atwork_only_CRs, tsne_features_jobs_df], axis = 1)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.scatterplot(x = 'tsne_0', y = 'tsne_1', data = tsne_features_jobs_df)\n",
    "plt.title(\"Projection of the data on 2 components + ground truth labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = engage_only['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    data_part = engage_only[engage_only['MitreID'] == part]\n",
    "    sns.countplot(x='wave_study_day', hue='kmeans_scaled_survey', data=data_part)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cluster 1 % to final data\n",
    "participants = data['MitreID'].unique()\n",
    "\n",
    "for part in participants:\n",
    "    if part in engage_only_features.index:\n",
    "        perc = engage_only_features[(engage_only_features.index == part) & (engage_only_features['kmeans_scaled_survey'] == 1)].shape[0] / engage_only_features[engage_only_features.index == part].shape[0]\n",
    "\n",
    "        data.loc[data['MitreID'] == part, 'engage_CR_perc'] = perc\n",
    "        engage_only.loc[engage_only['MitreID'] == part, 'engage_CR_perc'] = perc\n",
    "    \n",
    "    else:\n",
    "        data.loc[data['MitreID'] == part, 'engage_CR_perc'] = np.nan\n",
    "        engage_only.loc[engage_only['MitreID'] == part, 'engage_CR_perc'] = np.nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
